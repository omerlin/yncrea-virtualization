{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"rancherOS/","text":"Why RancherOS ? Several reasons : This is because Rancher is a cool company delivering KISS [^1] cloud tools For Kubernetes, we will see 2 good On Premise [^2] Rancher tools K3S a very light kubernetes dedicated to Edge [^3] Computing It really nice as you can start really easily a Kubernetes cluster on Raspberry PI RKE (Rancher Kubernetes Engine) the professional Kubernetes packaging easy to use [^1] Keep It Simple and Stupid [^2] \"On Premise\" means \"private\", we can say also \"In House\" [^3] Edge computing could be seen as a micro Data center (IOT / 5G) Note Another very cool company you should have a look to: Hashicorp Example of nice products: vagrant, packer, terraform, vault Links RancherOS release Rancher OS installation with dockerMachine Rancher OS overview LABS: installing RancherOS machines They are several objectives behind this LAB * Show you another virtual machine deployments * Explore and understand another modern packaging * Have machines ready to test docker * Have machines ready for a future Kubernetes cluster Instruction You need to install 2 machines named \"ROSServer\" and \"ROSWorker1\" Deployment instructions To deploy a RancherOS machine, you will need this command. (you will have to adapt it) $ docker-machine create -d virtualbox \\ --virtualbox-boot2docker-url https://releases.rancher.com/os/latest/rancheros.iso \\ --virtualbox-memory <MEMORY-SIZE> \\ <MACHINE-NAME> Warning Use Git Bash to execute the following commands ( MobaXterm won't work well here) Tip To avoid using too much bandwidth you can download the ISO locally (around 140Mb) and then change the parameter virtualbox-boot2docker-url value to refer a local file To check the VM are deployed: $ VBoxManage list runningvms | grep <MACHINE-NAME> Understand the installation Your turn Ask you some questions about the driver interface created ? What are the interface type ? Could you check your networks possibility ? Pinging machine (from your localhost, between the machines) Access to internet ? Tip To start the VirtualBox guest addition: ros service enable virtual-box ros service start virtual-box You need to reboot the VM to really activate the service RancherOS commands summary COMMAND DESCRIPTION docker Good old Docker, use that to run stuff. system-docker The Docker instance running the system containers. (root rights) ros Control and configure RancherOS","title":"Why RancherOS ?"},{"location":"rancherOS/#why-rancheros","text":"Several reasons : This is because Rancher is a cool company delivering KISS [^1] cloud tools For Kubernetes, we will see 2 good On Premise [^2] Rancher tools K3S a very light kubernetes dedicated to Edge [^3] Computing It really nice as you can start really easily a Kubernetes cluster on Raspberry PI RKE (Rancher Kubernetes Engine) the professional Kubernetes packaging easy to use [^1] Keep It Simple and Stupid [^2] \"On Premise\" means \"private\", we can say also \"In House\" [^3] Edge computing could be seen as a micro Data center (IOT / 5G) Note Another very cool company you should have a look to: Hashicorp Example of nice products: vagrant, packer, terraform, vault","title":"Why RancherOS ?"},{"location":"rancherOS/#links","text":"RancherOS release Rancher OS installation with dockerMachine Rancher OS overview","title":"Links"},{"location":"rancherOS/#labs-installing-rancheros-machines","text":"They are several objectives behind this LAB * Show you another virtual machine deployments * Explore and understand another modern packaging * Have machines ready to test docker * Have machines ready for a future Kubernetes cluster","title":"LABS: installing RancherOS machines"},{"location":"rancherOS/#instruction","text":"You need to install 2 machines named \"ROSServer\" and \"ROSWorker1\"","title":"Instruction"},{"location":"rancherOS/#deployment-instructions","text":"To deploy a RancherOS machine, you will need this command. (you will have to adapt it) $ docker-machine create -d virtualbox \\ --virtualbox-boot2docker-url https://releases.rancher.com/os/latest/rancheros.iso \\ --virtualbox-memory <MEMORY-SIZE> \\ <MACHINE-NAME> Warning Use Git Bash to execute the following commands ( MobaXterm won't work well here) Tip To avoid using too much bandwidth you can download the ISO locally (around 140Mb) and then change the parameter virtualbox-boot2docker-url value to refer a local file To check the VM are deployed: $ VBoxManage list runningvms | grep <MACHINE-NAME>","title":"Deployment instructions"},{"location":"rancherOS/#understand-the-installation","text":"Your turn Ask you some questions about the driver interface created ? What are the interface type ? Could you check your networks possibility ? Pinging machine (from your localhost, between the machines) Access to internet ? Tip To start the VirtualBox guest addition: ros service enable virtual-box ros service start virtual-box You need to reboot the VM to really activate the service","title":"Understand the installation"},{"location":"rancherOS/#rancheros-commands-summary","text":"COMMAND DESCRIPTION docker Good old Docker, use that to run stuff. system-docker The Docker instance running the system containers. (root rights) ros Control and configure RancherOS","title":"RancherOS commands summary"},{"location":"test/","text":"Footnotes 1 work like reference links They auto-number like ordered lists 2 You can use any reference id 3 like ref links they can be organized at bottom of paragraph or page. footnote, click the return icon here to go back -> \u21a9 the number will not necessarily be what you use \u21a9 text reference \u21a9","title":"Test"},{"location":"cheatsheets/choco/","text":"Chocolatey Cheatsheet Useful Links Setup Commands list List Installed Packages choco list --local-only Install a Package Standard Package Installation choco install PACKAGENAME -y choco install git -y Multiple Package Installation choco install packer vagrant virtualbox git poshgit chefdk visualstudiocode -y Ignore Checksums choco install github --ignore-checksums Upgrade a Package choco upgrade <pkg|all> [<pkg2> <pkgN>] [<options/switches>] choco upgrade git -y choco upgrade all -y Uninstall a Package choco uninstall PACKAGENAME choco uninstall git --all-versions -y List outdated packages choco outdated","title":"chocolatey"},{"location":"cheatsheets/choco/#chocolatey-cheatsheet","text":"","title":"Chocolatey Cheatsheet"},{"location":"cheatsheets/choco/#useful-links","text":"Setup Commands list","title":"Useful Links"},{"location":"cheatsheets/choco/#list-installed-packages","text":"choco list --local-only","title":"List Installed Packages"},{"location":"cheatsheets/choco/#install-a-package","text":"","title":"Install a Package"},{"location":"cheatsheets/choco/#standard-package-installation","text":"choco install PACKAGENAME -y choco install git -y","title":"Standard Package Installation"},{"location":"cheatsheets/choco/#multiple-package-installation","text":"choco install packer vagrant virtualbox git poshgit chefdk visualstudiocode -y","title":"Multiple Package Installation"},{"location":"cheatsheets/choco/#ignore-checksums","text":"choco install github --ignore-checksums","title":"Ignore Checksums"},{"location":"cheatsheets/choco/#upgrade-a-package","text":"choco upgrade <pkg|all> [<pkg2> <pkgN>] [<options/switches>] choco upgrade git -y choco upgrade all -y","title":"Upgrade a Package"},{"location":"cheatsheets/choco/#uninstall-a-package","text":"choco uninstall PACKAGENAME choco uninstall git --all-versions -y","title":"Uninstall a Package"},{"location":"cheatsheets/choco/#list-outdated-packages","text":"choco outdated","title":"List outdated packages"},{"location":"cheatsheets/docker/","text":"Docker cheat sheet We describe here the most common Docker usage Alternate links The ultimate Docker cheat sheet Commands detail Tip To have details on a particular command docker <command> --help Build Build an image from the Dockerfile in the current directory and tag the image docker build -t myimage:1.0 . List all images that are locally stored with the Docker Engine docker image ls Delete an image from the local image store docker image rm alpine:3.4 Share Pull an image from a registry docker pull myimage:1.0 Retag a local image with a new image name and tag docker tag myimage:1.0 myrepo/myimage:2.0 Push an image to a registry docker push myrepo/myimage:2.0 Run Run a container from the Alpine version 3.9 image, name the running container \u201cweb\u201d and expose port 5000 externally, mapped to port 80 inside the container. docker container run --name web -p 5000:80 alpine:3.9 Stop a running container through SIGTERM docker container stop web Stop a running container through SIGKILL docker container kill web List the networks docker network ls List the running containers (add --all to include stopped containers) docker container ls Delete all running and stopped containers docker container rm -f $(docker ps -aq) Print the last 100 lines of a container\u2019s logs docker container logs --tail 100 web","title":"docker"},{"location":"cheatsheets/docker/#docker-cheat-sheet","text":"We describe here the most common Docker usage","title":"Docker cheat sheet"},{"location":"cheatsheets/docker/#alternate-links","text":"The ultimate Docker cheat sheet","title":"Alternate links"},{"location":"cheatsheets/docker/#commands-detail","text":"Tip To have details on a particular command docker <command> --help","title":"Commands detail"},{"location":"cheatsheets/docker/#build","text":"Build an image from the Dockerfile in the current directory and tag the image docker build -t myimage:1.0 . List all images that are locally stored with the Docker Engine docker image ls Delete an image from the local image store docker image rm alpine:3.4","title":"Build"},{"location":"cheatsheets/docker/#share","text":"Pull an image from a registry docker pull myimage:1.0 Retag a local image with a new image name and tag docker tag myimage:1.0 myrepo/myimage:2.0 Push an image to a registry docker push myrepo/myimage:2.0","title":"Share"},{"location":"cheatsheets/docker/#run","text":"Run a container from the Alpine version 3.9 image, name the running container \u201cweb\u201d and expose port 5000 externally, mapped to port 80 inside the container. docker container run --name web -p 5000:80 alpine:3.9 Stop a running container through SIGTERM docker container stop web Stop a running container through SIGKILL docker container kill web List the networks docker network ls List the running containers (add --all to include stopped containers) docker container ls Delete all running and stopped containers docker container rm -f $(docker ps -aq) Print the last 100 lines of a container\u2019s logs docker container logs --tail 100 web","title":"Run"},{"location":"cheatsheets/kubernetes/","text":"Kubernetes Cheatsheet Useful Links The most complete reference : Official kubectl Cheat Sheet My favourite to debug kubernetes services Condensed cheat sheet Get information about all running pods: kubectl get pods get pods, and also show labels attached to those pods kubectl get pods --show-labels Get all pods on all namespaces kubectl get pods --all-namespaces Describe one pod kubectl describe pod <pod> Expose the port of a pod (creates a new service) kubectl expose pod <pod> --port=444 --name=frontend Port forward the exposed pod port to your local machine kubectl port-forward <pod> 8080 Attach to the pod kubectl attach <podname> -i Execute a command on the pod kubectl exec <pod> -- command Add a new label to a pod kubectl label pods <pod> mylabel=awesome Run a shell in a pod - very useful for debugging kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Get information on current deployments kubectl get deployments Get information about the replica sets kubectl get rs Get deployment status kubectl rollout status deployment/helloworld-deployment Run k8s-demo with the image label version 2 kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 Edit the deployment object kubectl edit deployment/helloworld-deployment Get the status of the rollout kubectl rollout status deployment/helloworld-deployment Get the rollout history kubectl rollout history deployment/helloworld-deployment Rollback to previous version kubectl rollout undo deployment/helloworld-deployment Rollback to any version version kubectl rollout undo deployment/helloworld-deployment --to-revision=n Abbreviations used Resource type alias configmaps cm customresourcedefinition crd daemonsets ds horizontalpodautoscalers hpa ingres ing limitranges limits Namespace ns nodes no persistentvolumeclaims pvc persistentvolumes pv pods po replicasets rs replicationcontrollers rc resourcequotas quota","title":"kubernetes"},{"location":"cheatsheets/kubernetes/#kubernetes-cheatsheet","text":"","title":"Kubernetes Cheatsheet"},{"location":"cheatsheets/kubernetes/#useful-links","text":"The most complete reference : Official kubectl Cheat Sheet My favourite to debug kubernetes services","title":"Useful Links"},{"location":"cheatsheets/kubernetes/#condensed-cheat-sheet","text":"Get information about all running pods: kubectl get pods get pods, and also show labels attached to those pods kubectl get pods --show-labels Get all pods on all namespaces kubectl get pods --all-namespaces Describe one pod kubectl describe pod <pod> Expose the port of a pod (creates a new service) kubectl expose pod <pod> --port=444 --name=frontend Port forward the exposed pod port to your local machine kubectl port-forward <pod> 8080 Attach to the pod kubectl attach <podname> -i Execute a command on the pod kubectl exec <pod> -- command Add a new label to a pod kubectl label pods <pod> mylabel=awesome Run a shell in a pod - very useful for debugging kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Get information on current deployments kubectl get deployments Get information about the replica sets kubectl get rs Get deployment status kubectl rollout status deployment/helloworld-deployment Run k8s-demo with the image label version 2 kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 Edit the deployment object kubectl edit deployment/helloworld-deployment Get the status of the rollout kubectl rollout status deployment/helloworld-deployment Get the rollout history kubectl rollout history deployment/helloworld-deployment Rollback to previous version kubectl rollout undo deployment/helloworld-deployment Rollback to any version version kubectl rollout undo deployment/helloworld-deployment --to-revision=n","title":"Condensed cheat sheet"},{"location":"cheatsheets/kubernetes/#abbreviations-used","text":"Resource type alias configmaps cm customresourcedefinition crd daemonsets ds horizontalpodautoscalers hpa ingres ing limitranges limits Namespace ns nodes no persistentvolumeclaims pvc persistentvolumes pv pods po replicasets rs replicationcontrollers rc resourcequotas quota","title":"Abbreviations used"},{"location":"cheatsheets/vagrant/","text":"Vagrant Cheatsheet Typing vagrant from the command line will display a list of all available commands. Be sure that you are in the same directory as the Vagrantfile when running these commands! Creating a VM vagrant init -- Initialize Vagrant with a Vagrantfile and ./.vagrant directory, using no specified base image. Before you can do vagrant up, you'll need to specify a base image in the Vagrantfile. vagrant init <boxpath> -- Initialize Vagrant with a specific box. To find a box, go to the public Vagrant box catalog . When you find one you like, just replace it's name with boxpath. For example, vagrant init ubuntu/trusty64 . Starting a VM vagrant up -- starts vagrant environment (also provisions only on the FIRST vagrant up) vagrant resume -- resume a suspended machine (vagrant up works just fine for this as well) vagrant provision -- forces reprovisioning of the vagrant machine vagrant reload -- restarts vagrant machine, loads new Vagrantfile configuration vagrant reload --provision -- restart the virtual machine and force provisioning Getting into a VM vagrant ssh -- connects to machine via SSH vagrant ssh <boxname> -- If you give your box a name in your Vagrantfile, you can ssh into it with boxname. Works from any directory. Stopping a VM vagrant halt -- stops the vagrant machine vagrant suspend -- suspends a virtual machine (remembers state) Cleaning Up a VM vagrant destroy -- stops and deletes all traces of the vagrant machine vagrant destroy -f -- same as above, without confirmation Boxes vagrant box list -- see a list of all installed boxes on your computer vagrant box add <name> <url> -- download a box image to your computer vagrant box outdated -- check for updates vagrant box update vagrant boxes remove <name> -- deletes a box from the machine vagrant package -- packages a running virtualbox env in a reusable box Saving Progress - vagrant snapshot save [options] [vm-name] <name> -- vm-name is often default . Allows us to save so that we can rollback at a later time Tips vagrant -v -- get the vagrant version vagrant status -- outputs status of the vagrant machine vagrant global-status -- outputs status of all vagrant machines vagrant global-status --prune -- same as above, but prunes invalid entries vagrant provision --debug -- use the debug flag to increase the verbosity of the output vagrant push -- yes, vagrant can be configured to deploy code ! vagrant up --provision | tee provision.log -- Runs vagrant up , forces provisioning and logs all output to a file","title":"vagrant"},{"location":"cheatsheets/vagrant/#vagrant-cheatsheet","text":"Typing vagrant from the command line will display a list of all available commands. Be sure that you are in the same directory as the Vagrantfile when running these commands!","title":"Vagrant Cheatsheet"},{"location":"cheatsheets/vagrant/#creating-a-vm","text":"vagrant init -- Initialize Vagrant with a Vagrantfile and ./.vagrant directory, using no specified base image. Before you can do vagrant up, you'll need to specify a base image in the Vagrantfile. vagrant init <boxpath> -- Initialize Vagrant with a specific box. To find a box, go to the public Vagrant box catalog . When you find one you like, just replace it's name with boxpath. For example, vagrant init ubuntu/trusty64 .","title":"Creating a VM"},{"location":"cheatsheets/vagrant/#starting-a-vm","text":"vagrant up -- starts vagrant environment (also provisions only on the FIRST vagrant up) vagrant resume -- resume a suspended machine (vagrant up works just fine for this as well) vagrant provision -- forces reprovisioning of the vagrant machine vagrant reload -- restarts vagrant machine, loads new Vagrantfile configuration vagrant reload --provision -- restart the virtual machine and force provisioning","title":"Starting a VM"},{"location":"cheatsheets/vagrant/#getting-into-a-vm","text":"vagrant ssh -- connects to machine via SSH vagrant ssh <boxname> -- If you give your box a name in your Vagrantfile, you can ssh into it with boxname. Works from any directory.","title":"Getting into a VM"},{"location":"cheatsheets/vagrant/#stopping-a-vm","text":"vagrant halt -- stops the vagrant machine vagrant suspend -- suspends a virtual machine (remembers state)","title":"Stopping a VM"},{"location":"cheatsheets/vagrant/#cleaning-up-a-vm","text":"vagrant destroy -- stops and deletes all traces of the vagrant machine vagrant destroy -f -- same as above, without confirmation","title":"Cleaning Up a VM"},{"location":"cheatsheets/vagrant/#boxes","text":"vagrant box list -- see a list of all installed boxes on your computer vagrant box add <name> <url> -- download a box image to your computer vagrant box outdated -- check for updates vagrant box update vagrant boxes remove <name> -- deletes a box from the machine vagrant package -- packages a running virtualbox env in a reusable box","title":"Boxes"},{"location":"cheatsheets/vagrant/#saving-progress","text":"- vagrant snapshot save [options] [vm-name] <name> -- vm-name is often default . Allows us to save so that we can rollback at a later time","title":"Saving Progress"},{"location":"cheatsheets/vagrant/#tips","text":"vagrant -v -- get the vagrant version vagrant status -- outputs status of the vagrant machine vagrant global-status -- outputs status of all vagrant machines vagrant global-status --prune -- same as above, but prunes invalid entries vagrant provision --debug -- use the debug flag to increase the verbosity of the output vagrant push -- yes, vagrant can be configured to deploy code ! vagrant up --provision | tee provision.log -- Runs vagrant up , forces provisioning and logs all output to a file","title":"Tips"},{"location":"cheatsheets/vboxManage/","text":"VirtualBox cheatsheet The CLI tool to do all Virtualbox operations: VBoxManage Useful links VboxManage reference guide Tip Add your Virtualbox location to your environment PATH List VMs VBoxManage list vms Running VMs : vboxmanage list runningvms Info Includes hidden VMs created by Vagrant Start VM In headless mode VBoxManage startvm worker1 --type headless Stop VM Clean way to stop machines: vboxmanage controlvm worker1 poweroff soft Snapshot VM VBoxManage snapshot worker1 take snap-worker1-initial --description=\"initial state\" List snapshot VBoxManage snapshot worker1 list Restore a snapshot VBoxManage snapshot { uuid|vmname } restore { snapshot-name } Delete VM VBoxManage unregistervm VMNAME --delete List forwarded VM ports VBoxManage showvminfo VMNAME --machinereadable | Select-String -Pattern 'Forwarding' Note Select-String is a PowerShell command VirtualBox Help Simply type VBoxManage - You will see how rich is this CLI command You can do all that is possible with the GUI and more ...","title":"virtualBoxManage"},{"location":"cheatsheets/vboxManage/#virtualbox-cheatsheet","text":"The CLI tool to do all Virtualbox operations: VBoxManage","title":"VirtualBox cheatsheet"},{"location":"cheatsheets/vboxManage/#useful-links","text":"VboxManage reference guide Tip Add your Virtualbox location to your environment PATH","title":"Useful links"},{"location":"cheatsheets/vboxManage/#list-vms","text":"VBoxManage list vms Running VMs : vboxmanage list runningvms Info Includes hidden VMs created by Vagrant","title":"List VMs"},{"location":"cheatsheets/vboxManage/#start-vm","text":"In headless mode VBoxManage startvm worker1 --type headless","title":"Start VM"},{"location":"cheatsheets/vboxManage/#stop-vm","text":"Clean way to stop machines: vboxmanage controlvm worker1 poweroff soft","title":"Stop VM"},{"location":"cheatsheets/vboxManage/#snapshot-vm","text":"VBoxManage snapshot worker1 take snap-worker1-initial --description=\"initial state\"","title":"Snapshot VM"},{"location":"cheatsheets/vboxManage/#list-snapshot","text":"VBoxManage snapshot worker1 list","title":"List snapshot"},{"location":"cheatsheets/vboxManage/#restore-a-snapshot","text":"VBoxManage snapshot { uuid|vmname } restore { snapshot-name }","title":"Restore a snapshot"},{"location":"cheatsheets/vboxManage/#delete-vm","text":"VBoxManage unregistervm VMNAME --delete","title":"Delete VM"},{"location":"cheatsheets/vboxManage/#list-forwarded-vm-ports","text":"VBoxManage showvminfo VMNAME --machinereadable | Select-String -Pattern 'Forwarding' Note Select-String is a PowerShell command","title":"List forwarded VM ports"},{"location":"cheatsheets/vboxManage/#virtualbox-help","text":"Simply type VBoxManage - You will see how rich is this CLI command You can do all that is possible with the GUI and more ...","title":"VirtualBox Help"},{"location":"cheatsheets/wsl2/","text":"To list installed distributions wsl -l wsl --list To list installed distributions along with its running status and wsl config being 1 or 2 wsl -l --verbose wsl -l -v To run a specific distro wsl -d distro_name wsl --distribution distro_name To terminate/shutdown a specific distro wsl -t distro_name_to_shutdown wsl --terminate distro_name_to_shutdown To shutdown all disstros wsl --shutdown Set specific distro as default wsl -s my_default_distro wsl --set-default my_default_distro To EXPORT a running distro as image wsl --export distro_name_to_export windows_path\\tar_file_name.tar To IMPORT an image as distro wsl --import new_distro_name install_location_windows_path tar_file_name.tar --version wsl-version-1-or-2 wsl --import Ubuntu-20 D:\\VMs\\WSL\\Ubuntu-20\\ Ubuntu-20.04.tar --version 2 ## Setting my secondary HDD as storate loc for new distro To UNREGISTER (also removes the its file storage) a distro wsl --unregister distro_name_that_delete To run a WSL distro as the specified user. wsl -u username -d distroname wsl -u root -d Ubuntu-20.04 To change the default user for a distribution distributionName config --default-user Username ubuntu config --default-user my_default_username ubuntu2004.exe config --default-user johndoe # When you have Ubuntu 20.04 version installed from the Microsoft Store","title":"WSL2"},{"location":"cheatsheets/wsl2/#to-list-installed-distributions","text":"wsl -l wsl --list","title":"To list installed distributions"},{"location":"cheatsheets/wsl2/#to-list-installed-distributions_1","text":"along with its running status and wsl config being 1 or 2 wsl -l --verbose wsl -l -v","title":"To list installed distributions"},{"location":"cheatsheets/wsl2/#to-run-a-specific-distro","text":"wsl -d distro_name wsl --distribution distro_name","title":"To run a specific distro"},{"location":"cheatsheets/wsl2/#to-terminateshutdown-a-specific-distro","text":"wsl -t distro_name_to_shutdown wsl --terminate distro_name_to_shutdown","title":"To terminate/shutdown a specific distro"},{"location":"cheatsheets/wsl2/#to-shutdown-all-disstros","text":"wsl --shutdown","title":"To shutdown all disstros"},{"location":"cheatsheets/wsl2/#set-specific-distro-as-default","text":"wsl -s my_default_distro wsl --set-default my_default_distro","title":"Set specific distro as default"},{"location":"cheatsheets/wsl2/#to-export-a-running-distro-as-image","text":"wsl --export distro_name_to_export windows_path\\tar_file_name.tar","title":"To EXPORT a running distro as image"},{"location":"cheatsheets/wsl2/#to-import-an-image-as-distro","text":"wsl --import new_distro_name install_location_windows_path tar_file_name.tar --version wsl-version-1-or-2 wsl --import Ubuntu-20 D:\\VMs\\WSL\\Ubuntu-20\\ Ubuntu-20.04.tar --version 2 ## Setting my secondary HDD as storate loc for new distro","title":"To IMPORT an image as distro"},{"location":"cheatsheets/wsl2/#to-unregister-also-removes-the-its-file-storage-a-distro","text":"wsl --unregister distro_name_that_delete","title":"To UNREGISTER (also removes the its file storage) a distro"},{"location":"cheatsheets/wsl2/#to-run-a-wsl-distro-as-the-specified-user","text":"wsl -u username -d distroname wsl -u root -d Ubuntu-20.04","title":"To run a WSL distro as the specified user."},{"location":"cheatsheets/wsl2/#to-change-the-default-user-for-a-distribution","text":"distributionName config --default-user Username ubuntu config --default-user my_default_username ubuntu2004.exe config --default-user johndoe # When you have Ubuntu 20.04 version installed from the Microsoft Store","title":"To change the default user for a distribution"},{"location":"day1/10_prerequisites/","text":"Requirements for labs All the pre-requisites to follow the labs System pre-requisites You need to be on Windows (7+), Linux or MacOS. VT-x/AMD-v virtualization must be enabled in BIOS check with Windows 10 task manager Note We will need Windows 10 for Wsl2 and Docker Desktop in future sections Warning You need the Administrative rights to be able to install software Don't forget to switch to admin command on Windows Windows recommended pre-requisite We recommend installing chocolatey Windows package manager to make it easy. Read this Why chocolatey If you are convinced, please follow this: Install chocolatey To see software installed locally: choco list -l To search a package remotely: choco search <package_name> For people allergic to command prompt ( bad for automation from my point of view ), there is a chocolaty GUI. Software needed You need to install this list of software Vagrant - to build and manage virtual machines easily. This will \"pilot\" virtualbox. Oracle Virtualbox - the famous Type 2 hypervisor (Windows only) Mobaxterm - a beautiful cygwin package toolbox - an alternative to putty Windows installation We do recommend also to install git, to clone source on your computer. choco install -y mobaxterm vagrant git Note We advise installing virtualbox manually instead of using chocolatey Url: Virtualbox download Issues Windows 11 family issue Links: vt-x-enabled-in-bios-but-not-working-in-windows-11 Seems the solution is to: disable Core isolation in Windows Security To disable core isolation: vt-x-enabled-in-bios-but-not-working-in-windows-11","title":"Env - Prerequisites"},{"location":"day1/10_prerequisites/#requirements-for-labs","text":"All the pre-requisites to follow the labs","title":"Requirements for labs"},{"location":"day1/10_prerequisites/#system-pre-requisites","text":"You need to be on Windows (7+), Linux or MacOS. VT-x/AMD-v virtualization must be enabled in BIOS","title":"System pre-requisites"},{"location":"day1/10_prerequisites/#check-with-windows-10-task-manager","text":"Note We will need Windows 10 for Wsl2 and Docker Desktop in future sections Warning You need the Administrative rights to be able to install software Don't forget to switch to admin command on Windows","title":"check with Windows 10 task manager"},{"location":"day1/10_prerequisites/#windows-recommended-pre-requisite","text":"We recommend installing chocolatey Windows package manager to make it easy. Read this Why chocolatey If you are convinced, please follow this: Install chocolatey To see software installed locally: choco list -l To search a package remotely: choco search <package_name> For people allergic to command prompt ( bad for automation from my point of view ), there is a chocolaty GUI.","title":"Windows recommended pre-requisite"},{"location":"day1/10_prerequisites/#software-needed","text":"You need to install this list of software Vagrant - to build and manage virtual machines easily. This will \"pilot\" virtualbox. Oracle Virtualbox - the famous Type 2 hypervisor (Windows only) Mobaxterm - a beautiful cygwin package toolbox - an alternative to putty","title":"Software needed"},{"location":"day1/10_prerequisites/#windows-installation","text":"We do recommend also to install git, to clone source on your computer. choco install -y mobaxterm vagrant git Note We advise installing virtualbox manually instead of using chocolatey Url: Virtualbox download","title":"Windows installation"},{"location":"day1/10_prerequisites/#issues","text":"","title":"Issues"},{"location":"day1/10_prerequisites/#windows-11-family-issue","text":"Links: vt-x-enabled-in-bios-but-not-working-in-windows-11 Seems the solution is to: disable Core isolation in Windows Security To disable core isolation: vt-x-enabled-in-bios-but-not-working-in-windows-11","title":"Windows 11 family issue"},{"location":"day1/12_windows_prerequisites/","text":"Windows prerequisites Since Windows 10, we have the possibility to activate a Windows SubSystem based on Linux (WSL). WSL 1 was activated using Hyper-V (The Layer 1 Windows virtualization). WSL 2 has arrived and could be more seen as an embedded container using Hyper-V librairies (and is no more an Hyper-V VM). Hyper-V was also not working on Windows Home edition (which is quite a limitation !) The idea behind is to have something leaner (lighter) and faster with performance optimization. I let you read around this topic in Microsoft official documentations (and other independent forum - but take care this is kinde jungle here ) Tip We configure WSL 2 to benefit from Docker Desktop While this is a cool feature and really simplifies a lot development it has some limitations too ... This is why we will not focus only on it but first work on VirtualBox. Warning The Linux OS you install do not start a full subsystem (no init nor systemd scheduler) There are workaround of course - but this is something to put in place ( supervisord for instance ) But there are plenty cool usages like: configuration tool like \"ansible\" for instance (See later) bash scripting (faster than git-bash) development, compiling, image building (packer) ... The Wsl2 installation depends on your Windows version. Check your version of windows winver Warning Take care to use official documentations whenever you can because documentations and versions release are moving fast ! And you have a lot of \"bad\" or \"obsolete\" information. (The reverse is true also - sometimes official doumentation is unclear :smile) if recent distribution Recents means: Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11 The install is then automatic. Install on recent Windows 10/11 distribution if older version ... but not too old. You need at least: Windows 10 version 1909 (Build 18363 and higher) Install on Windows 10 distribution manually Windows features activation You can see windows features enabled in settings: Hyper-V services must be activated: WSL2 is activated:","title":"Env - Windows WSL2"},{"location":"day1/12_windows_prerequisites/#windows-prerequisites","text":"Since Windows 10, we have the possibility to activate a Windows SubSystem based on Linux (WSL). WSL 1 was activated using Hyper-V (The Layer 1 Windows virtualization). WSL 2 has arrived and could be more seen as an embedded container using Hyper-V librairies (and is no more an Hyper-V VM). Hyper-V was also not working on Windows Home edition (which is quite a limitation !) The idea behind is to have something leaner (lighter) and faster with performance optimization. I let you read around this topic in Microsoft official documentations (and other independent forum - but take care this is kinde jungle here ) Tip We configure WSL 2 to benefit from Docker Desktop While this is a cool feature and really simplifies a lot development it has some limitations too ... This is why we will not focus only on it but first work on VirtualBox. Warning The Linux OS you install do not start a full subsystem (no init nor systemd scheduler) There are workaround of course - but this is something to put in place ( supervisord for instance ) But there are plenty cool usages like: configuration tool like \"ansible\" for instance (See later) bash scripting (faster than git-bash) development, compiling, image building (packer) ... The Wsl2 installation depends on your Windows version.","title":"Windows prerequisites"},{"location":"day1/12_windows_prerequisites/#check-your-version-of-windows","text":"winver Warning Take care to use official documentations whenever you can because documentations and versions release are moving fast ! And you have a lot of \"bad\" or \"obsolete\" information. (The reverse is true also - sometimes official doumentation is unclear :smile)","title":"Check your version of windows"},{"location":"day1/12_windows_prerequisites/#if-recent-distribution","text":"Recents means: Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11 The install is then automatic. Install on recent Windows 10/11 distribution","title":"if recent distribution"},{"location":"day1/12_windows_prerequisites/#if-older-version","text":"... but not too old. You need at least: Windows 10 version 1909 (Build 18363 and higher) Install on Windows 10 distribution manually","title":"if older version"},{"location":"day1/12_windows_prerequisites/#windows-features-activation","text":"You can see windows features enabled in settings: Hyper-V services must be activated: WSL2 is activated:","title":"Windows features activation"},{"location":"day1/15_story_of_virtualization/","text":"The \"long\" story of virtualization ... What and why Main reasons to adopt virtualization fot IT administrators & Management Average 5-10 % of real resources used on traditional HW Hardware failure Single OS on top of Hardware IT managers duties: Security Hw & Sw maintenance Hw Cooling and power management Where are we ? In the late 1990s, VMware introduced a technology that enabled most of the code to execute directly on the CPU without the requirement for translation or emulation. The concept of the \u201chypervisor\u201d \u2013 a platform upon which IT could create and run virtual machines comes from Mainframes For years, VMware and its patents ruled the realm of virtualization. On the server side, running on bare metal, VMware's ESX became the leading Type 1 (or native) hypervisor. On the client side, running within an existing desktop operating system, VMware Workstation was among the top Type 2 (or hosted) hypervisors. Over the years, some interesting open-source projects emerged, including Xen and Quick EMUlator (QEMU). Neither was as fast or as flexible as VMware, but they set a foundation that would prove worthy down the road. Around 2005, Advanced Micro Devices (AMD) and Intel created new processor extensions to the x86 architecture. These extensions provided hardware assistance for dealing with privileged instructions. Called AMD-V and VT-x by AMD and Intel respectively, these extensions changed the landscape, eventually opening server virtualization to new players. For more details on Xen and KVM: Xen and KVM Even Microsoft eventually got into the game with the release of Hyper-V in 2008. (Archi similar to Xen) When virtualization essentially became free, or at least accessible without expensive licensing fees, new use cases came to light. Most notably, Amazon began to use the Xen platform to rent some of its excess computing capacity to thirdparty customers. Through their application programming interfaces (APIs), Amazon kicked off the revolution of elastic cloud computing, where the applications could self-provision resources to fit their workloads. Progressively, open-source hypervisors have matured and become pervasive in cloud computing. Technology vendors developing solutions for virtual environments are increasingly required to support all major hypervisors (Xen and KVM) With this hypervisor parity, innovation became focused on the private/public cloud hardware architectures and the software ecosystems that surround them: storage architectures, softwaredefined networking, intelligent and autonomous orchestration, and application APIs. This leads to new actors like Nutanix ant its HCI (Hyper Convergence Infrastructure) to emerge. Legacy server applications are slowly retiring to give way to elastic, self-defining cloud applications (although they will coexist side by side for some time). Server and Desktop Virtualization Desktop Virtualization is a response to increasing numbers of employees working remotely and from multiple devices. Server virtualization is an answer for companies that need to diversify workloads and maximize server efficiency Terminology Hardware Abstraction Layer Host Operating System Guest Operating System Partitioning, isolation, sharing of resources Images lifecycle Virtual Network Hypervisors A hypervisor provides software to manage virtual machine access to the underlying hardware. The hypervisor creates, manages, and monitors virtual machines. There are 2 types: Type 1 (Bare metal) and Type 2 (Hosted) . Hypervisors come with Management Software that: Controls the failover Authorize dynamic over-allocation of resources (Type 1 only, can specify static too) Manages the VMs lifecycle (start/stop) Manages the VMs images location (lift and shift) Note The term Lift and Shift is used for migration from OnPremise Infrastructure to Cloud Infrastructure Another approach is Transform and Move Type 1 Hypervisor Info Citrix Xen Server (free), VMware vSphere (former VMware ESXi and VMware ESX), Microsoft Hyper-V Server, Parallels Server Bare Metal, Oracle vm server (free). Type 2 Hypervisor Info Microsoft (Microsoft VirtualPC, Microsoft Virtual Server), Parallels (Parallels Desktop, Parallels Server), Oracle VM VirtualBox (free), VMware (VMware Fusion, VMware Player, VMware Server, VMware Workstation), free (QEMU : x86 emulator), KVM (free) Containers Info Linux-VServer (isolating processes into user spaces) ; chroot (isolating the change of root) ; BSD Jail ; LXC : free; Docker.","title":"Virtualization - Story and Concepts"},{"location":"day1/15_story_of_virtualization/#the-long-story-of-virtualization","text":"","title":"The \"long\" story of virtualization ..."},{"location":"day1/15_story_of_virtualization/#what-and-why","text":"Main reasons to adopt virtualization fot IT administrators & Management Average 5-10 % of real resources used on traditional HW Hardware failure Single OS on top of Hardware IT managers duties: Security Hw & Sw maintenance Hw Cooling and power management","title":"What and why"},{"location":"day1/15_story_of_virtualization/#where-are-we","text":"In the late 1990s, VMware introduced a technology that enabled most of the code to execute directly on the CPU without the requirement for translation or emulation. The concept of the \u201chypervisor\u201d \u2013 a platform upon which IT could create and run virtual machines comes from Mainframes For years, VMware and its patents ruled the realm of virtualization. On the server side, running on bare metal, VMware's ESX became the leading Type 1 (or native) hypervisor. On the client side, running within an existing desktop operating system, VMware Workstation was among the top Type 2 (or hosted) hypervisors. Over the years, some interesting open-source projects emerged, including Xen and Quick EMUlator (QEMU). Neither was as fast or as flexible as VMware, but they set a foundation that would prove worthy down the road. Around 2005, Advanced Micro Devices (AMD) and Intel created new processor extensions to the x86 architecture. These extensions provided hardware assistance for dealing with privileged instructions. Called AMD-V and VT-x by AMD and Intel respectively, these extensions changed the landscape, eventually opening server virtualization to new players. For more details on Xen and KVM: Xen and KVM Even Microsoft eventually got into the game with the release of Hyper-V in 2008. (Archi similar to Xen) When virtualization essentially became free, or at least accessible without expensive licensing fees, new use cases came to light. Most notably, Amazon began to use the Xen platform to rent some of its excess computing capacity to thirdparty customers. Through their application programming interfaces (APIs), Amazon kicked off the revolution of elastic cloud computing, where the applications could self-provision resources to fit their workloads. Progressively, open-source hypervisors have matured and become pervasive in cloud computing. Technology vendors developing solutions for virtual environments are increasingly required to support all major hypervisors (Xen and KVM) With this hypervisor parity, innovation became focused on the private/public cloud hardware architectures and the software ecosystems that surround them: storage architectures, softwaredefined networking, intelligent and autonomous orchestration, and application APIs. This leads to new actors like Nutanix ant its HCI (Hyper Convergence Infrastructure) to emerge. Legacy server applications are slowly retiring to give way to elastic, self-defining cloud applications (although they will coexist side by side for some time).","title":"Where are we ?"},{"location":"day1/15_story_of_virtualization/#server-and-desktop-virtualization","text":"Desktop Virtualization is a response to increasing numbers of employees working remotely and from multiple devices. Server virtualization is an answer for companies that need to diversify workloads and maximize server efficiency","title":"Server and Desktop Virtualization"},{"location":"day1/15_story_of_virtualization/#terminology","text":"Hardware Abstraction Layer Host Operating System Guest Operating System Partitioning, isolation, sharing of resources Images lifecycle Virtual Network","title":"Terminology"},{"location":"day1/15_story_of_virtualization/#hypervisors","text":"A hypervisor provides software to manage virtual machine access to the underlying hardware. The hypervisor creates, manages, and monitors virtual machines. There are 2 types: Type 1 (Bare metal) and Type 2 (Hosted) . Hypervisors come with Management Software that: Controls the failover Authorize dynamic over-allocation of resources (Type 1 only, can specify static too) Manages the VMs lifecycle (start/stop) Manages the VMs images location (lift and shift) Note The term Lift and Shift is used for migration from OnPremise Infrastructure to Cloud Infrastructure Another approach is Transform and Move","title":"Hypervisors"},{"location":"day1/15_story_of_virtualization/#type-1-hypervisor","text":"Info Citrix Xen Server (free), VMware vSphere (former VMware ESXi and VMware ESX), Microsoft Hyper-V Server, Parallels Server Bare Metal, Oracle vm server (free).","title":"Type 1 Hypervisor"},{"location":"day1/15_story_of_virtualization/#type-2-hypervisor","text":"Info Microsoft (Microsoft VirtualPC, Microsoft Virtual Server), Parallels (Parallels Desktop, Parallels Server), Oracle VM VirtualBox (free), VMware (VMware Fusion, VMware Player, VMware Server, VMware Workstation), free (QEMU : x86 emulator), KVM (free)","title":"Type 2 Hypervisor"},{"location":"day1/15_story_of_virtualization/#containers","text":"Info Linux-VServer (isolating processes into user spaces) ; chroot (isolating the change of root) ; BSD Jail ; LXC : free; Docker.","title":"Containers"},{"location":"day1/20_virtualBox/","text":"Virtualbox software Virtualbox network configuration Comes from Virtualbox Network configuration Configure the NAT network Note RFC 1918 - the following IP ranges are reserved for private networks 10.0.0.0/8 \u2013 10.255.255.255 172.16.0.0/12 - 172.31.255.255 192.168.0.0/16 - 192.168.255.255 They are not routable on global internet Here a good online tool to calculate network: https://www.ipaddressguide.com/cidr","title":"Virtualization - Virtual Box"},{"location":"day1/20_virtualBox/#virtualbox-software","text":"","title":"Virtualbox software"},{"location":"day1/20_virtualBox/#virtualbox-network-configuration","text":"Comes from Virtualbox Network configuration","title":"Virtualbox network configuration"},{"location":"day1/20_virtualBox/#configure-the-nat-network","text":"Note RFC 1918 - the following IP ranges are reserved for private networks 10.0.0.0/8 \u2013 10.255.255.255 172.16.0.0/12 - 172.31.255.255 192.168.0.0/16 - 192.168.255.255 They are not routable on global internet Here a good online tool to calculate network: https://www.ipaddressguide.com/cidr","title":"Configure the NAT network"},{"location":"day1/22_virtualBoxLabs/","text":"Objectives The objectives of this lab is helping you to master VirtualBox deployment on your Laptop. The local deployment could be useful to: Prototyping application locally without network latency Work on air gap environment (isolated for security reason or on to spare resources) Have a local CI Reduce resource usage (Desktop or Server) by leveraging your development host Important These labs will be reused for the future deployment, so don't miss it please ! Add VBOXManange to your user PATH This can be done by changing the \"PAth\" entry in your environment. This is most probably in \"C:\\Program files\\Oracle\\virtualbox\" Trick To reload env variables in an existing command or Powershell terminal Use: refreshenv.exe Vboxmanage vboxmanage showvminfo worker1 Look if VT-x options are activated Tip For a future use: Under git-bash or any WSL Linux machine (if virtualbox is added to Environment PATH variable) VBoxManage.exe showvminfo worker1 | grep NIC vboxmanage showvminfo controller - -details | select-string 'Nic 1 rule' Labs 1: Using Linux Box with Vagrant Step 1: Start a VM To create a VM using Vagrant you have several options. By default, you have Cloud base VM Box available By default, vagrant will go to internet to get the Vagrant Boxes. It will download the image only one times. Sometime we prefer to work OFFLINE for many reasons ( No access, security, network bandwidth ... ) We will first download the box buster64 image ... then integrates it locally using vagrant box You have just to do something like: vagrant box add buster64 file : /// $Env:USERPROFILE / Downloads / vagrant box list Then, create your own directory, for instance %USERPROFILE%/MyApp/vagrant. cd $Env:USERPROFILE / MyApp / vagrant # This command will create a Vagrantfile vagrant init buster64 vagrant up The first command vagrant init generate a Vagrantfile The second command vagrant up deploy/instantiate the VM. Note The default machine configuration may use a lot of ressources (memory / CPU ) ... take care. Note Vagrant networks options are quite limited on Virtualbox ( Nat and intnet ) To manage a Nat Network with vagrant: Vagrant virtual networking Remark if you have issues with proxy set http_proxy=http://user:password@host:port set https_proxy=https://user:password@host:port vagrant plugin install vagrant-proxyconf then set VAGRANT_HTTP_PROXY=%http_proxy% set VAGRANT_HTTPS_PROXY=%https_proxy% set VAGRANT_NO_PROXY=\"127.0.0.1\" vagrant up Step 2: access the VM using ssh You have several way to access the VM using SSH. The easiest way is by doing a: # this will be \"default\" if we don't define the name vagrant status vagrant ssh default The second way is by finding where the private key is located and then create a config file in your $HOME/.ssh directory. Alternative : you can use this labs correction: https://github.com/omerlin/yncrea-virtualization-labs https://www.vagrantup.com/docs/vagrantfile/machine_settings Labs 2: create a local Network of Linux Box You need to start 2 VM, create a Nat Network and check : VM are reachable on the Nat network The VM can access to internet You can reach the 2 guest VM from the host using ssh alias Correction See: https://github.com/omerlin/yncrea-virtualization-labs You have in the TwoBoxes the Vagrant file ssh vagrant@127.0.0.1 -p 2200 ssh vagrant@127.0.0.1 -p 2222 Integrates all together We will configure the 3 machines - so they belongs to the same internal network and are connected using Mobaxterm. To see the vagrant environments (and clean old one): vagrant global-status --prune Then you can find the \"private_key\" of each vagrant VMs using a Unix (or Windows) \"find\" command.","title":"Virtualization - Labs"},{"location":"day1/22_virtualBoxLabs/#objectives","text":"The objectives of this lab is helping you to master VirtualBox deployment on your Laptop. The local deployment could be useful to: Prototyping application locally without network latency Work on air gap environment (isolated for security reason or on to spare resources) Have a local CI Reduce resource usage (Desktop or Server) by leveraging your development host Important These labs will be reused for the future deployment, so don't miss it please !","title":"Objectives"},{"location":"day1/22_virtualBoxLabs/#add-vboxmanange-to-your-user-path","text":"This can be done by changing the \"PAth\" entry in your environment. This is most probably in \"C:\\Program files\\Oracle\\virtualbox\" Trick To reload env variables in an existing command or Powershell terminal Use: refreshenv.exe","title":"Add VBOXManange to your user PATH"},{"location":"day1/22_virtualBoxLabs/#vboxmanage","text":"vboxmanage showvminfo worker1 Look if VT-x options are activated Tip For a future use: Under git-bash or any WSL Linux machine (if virtualbox is added to Environment PATH variable) VBoxManage.exe showvminfo worker1 | grep NIC vboxmanage showvminfo controller - -details | select-string 'Nic 1 rule'","title":"Vboxmanage"},{"location":"day1/22_virtualBoxLabs/#labs-1-using-linux-box-with-vagrant","text":"","title":"Labs 1: Using Linux Box with Vagrant"},{"location":"day1/22_virtualBoxLabs/#step-1-start-a-vm","text":"To create a VM using Vagrant you have several options. By default, you have Cloud base VM Box available By default, vagrant will go to internet to get the Vagrant Boxes. It will download the image only one times. Sometime we prefer to work OFFLINE for many reasons ( No access, security, network bandwidth ... ) We will first download the box buster64 image ... then integrates it locally using vagrant box You have just to do something like: vagrant box add buster64 file : /// $Env:USERPROFILE / Downloads / vagrant box list Then, create your own directory, for instance %USERPROFILE%/MyApp/vagrant. cd $Env:USERPROFILE / MyApp / vagrant # This command will create a Vagrantfile vagrant init buster64 vagrant up The first command vagrant init generate a Vagrantfile The second command vagrant up deploy/instantiate the VM. Note The default machine configuration may use a lot of ressources (memory / CPU ) ... take care. Note Vagrant networks options are quite limited on Virtualbox ( Nat and intnet ) To manage a Nat Network with vagrant: Vagrant virtual networking Remark if you have issues with proxy set http_proxy=http://user:password@host:port set https_proxy=https://user:password@host:port vagrant plugin install vagrant-proxyconf then set VAGRANT_HTTP_PROXY=%http_proxy% set VAGRANT_HTTPS_PROXY=%https_proxy% set VAGRANT_NO_PROXY=\"127.0.0.1\" vagrant up","title":"Step 1: Start a VM"},{"location":"day1/22_virtualBoxLabs/#step-2-access-the-vm-using-ssh","text":"You have several way to access the VM using SSH. The easiest way is by doing a: # this will be \"default\" if we don't define the name vagrant status vagrant ssh default The second way is by finding where the private key is located and then create a config file in your $HOME/.ssh directory.","title":"Step 2: access the VM using ssh"},{"location":"day1/22_virtualBoxLabs/#alternative","text":"you can use this labs correction: https://github.com/omerlin/yncrea-virtualization-labs https://www.vagrantup.com/docs/vagrantfile/machine_settings","title":"Alternative :"},{"location":"day1/22_virtualBoxLabs/#labs-2-create-a-local-network-of-linux-box","text":"You need to start 2 VM, create a Nat Network and check : VM are reachable on the Nat network The VM can access to internet You can reach the 2 guest VM from the host using ssh alias","title":"Labs 2: create a local Network of Linux Box"},{"location":"day1/22_virtualBoxLabs/#correction","text":"See: https://github.com/omerlin/yncrea-virtualization-labs You have in the TwoBoxes the Vagrant file ssh vagrant@127.0.0.1 -p 2200 ssh vagrant@127.0.0.1 -p 2222","title":"Correction"},{"location":"day1/22_virtualBoxLabs/#integrates-all-together","text":"We will configure the 3 machines - so they belongs to the same internal network and are connected using Mobaxterm. To see the vagrant environments (and clean old one): vagrant global-status --prune Then you can find the \"private_key\" of each vagrant VMs using a Unix (or Windows) \"find\" command.","title":"Integrates all together"},{"location":"day1/30_dockerMachine/","text":"What is Docker machine ? Docker engine When people say \u201cDocker\u201d they typically mean Docker Engine , the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client Docker machine Docker Machine is a tool for provisioning and managing your Dockerized hosts (hosts with Docker Engine on them) The Dockerized hosts themselves can be thought of, and are sometimes referred to as, managed \u201cmachines\u201d. Why using Docker Machine ? We want an efficient way to provision machine with Docker pre-installed We want a small (in size) Docker enabled OS (Ubuntu 12.04.x is the default) Installation Follow: Docker machine installation link $ if [[ ! -d \" $HOME /bin\" ]] ; then mkdir -p \" $HOME /bin\" ; fi && \\ curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-Windows-x86_64.exe > \" $HOME /bin/docker-machine.exe\" && \\ chmod +x \" $HOME /bin/docker-machine.exe\" Important To play the above script, you will need git-bash Why RancherOS ? Several reasons : This is because Rancher is a cool company delivering KISS 1 cloud tools For Kubernetes, we will see 2 good On Premise 2 Rancher tools K3S a very light kubernetes dedicated to Edge 3 Computing It really nice as you can start really easily a Kubernetes cluster on Raspberry PI RKE (Rancher Kubernetes Engine) the professional Kubernetes packaging easy to use Note Another very cool company you should have a look to: Hashicorp Example of nice products: vagrant, packer, terraform, vault Links RancherOS release Rancher OS installation with dockerMachine Rancher OS overview LABS: Installing RancherOS machines They are several objectives behind this LAB * Show you another virtual machine deployments * Explore and understand another modern packaging * Have machines ready to test docker * Have machines ready for a future Kubernetes cluster Instruction You need to install 2 machines named \"ROSServer\" and \"ROSWorker1\" Deployment instructions To deploy a RancherOS machine, you will need this command. (you will have to adapt it) $ docker-machine create -d virtualbox \\ --virtualbox-boot2docker-url https://releases.rancher.com/os/latest/rancheros.iso \\ --virtualbox-memory <MEMORY-SIZE> \\ <MACHINE-NAME> Warning Use Git Bash to execute the following commands ( MobaXterm won't work well here) To check the VM are deployed: $ VBoxManage list runningvms | grep <MACHINE-NAME> Understand the installation Your turn Ask you some questions about the driver interface created ? What are the interface type ? Could you check your networks possibility ? Pinging machine (from your localhost, between the machines) Access to internet ? Tip To start the VirtualBox guest addition: ros service enable virtual-box ros service start virtual-box You need to reboot the VM to really activate the service RancherOS commands summary COMMAND DESCRIPTION docker Good old Docker, use that to run stuff. system-docker The Docker instance running the system containers. (root rights) ros Control and configure RancherOS LABS: machine setup Changing console The machine is particular there is by default no persistence on the OS console You can see the available console with: ros console list Change to ubuntu: ros console switch ubuntu You will have to restart the session Now you will get data persistence in your home like a traditional OS Adding git for future labs We will need to have \"git\" on the machine # Add this alias to a ~/.bash_profile alias git = 'docker run -ti --rm -v /home/docker:/git bwits/docker-git-alpine' Note As you can see everything is docker ... ! Adding docker compose Before switching to kubernetes we would like to deploy complex things with docker. Let prepare the installation of docker-compose sudo wget https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m) -O /usr/bin/docker-compose sudo chmod +x /usr/bin/docker-compose Adding your own private key Use ssh-keygen command to add a private/public rsa key - so you can easily connect to your machine. (from MobaXterm for example) Tip Host rosserver HostName 127.0.0.1 User docker Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile ~/.ssh/id_ros_rsa IdentitiesOnly yes LogLevel FATAL Host rosworker1 HostName 127.0.0.1 User docker Port 2223 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile ~/.ssh/id_ros_rsa IdentitiesOnly yes Keep It Simple and Stupid \u21a9 \"On Premise\" means \"private\", we can say also \"In House\" \u21a9 Edge computing could be seen as a micro Data center (IOT / 5G) \u21a9","title":"What is Docker machine ?"},{"location":"day1/30_dockerMachine/#what-is-docker-machine","text":"","title":"What is Docker machine ?"},{"location":"day1/30_dockerMachine/#docker-engine","text":"When people say \u201cDocker\u201d they typically mean Docker Engine , the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client","title":"Docker engine"},{"location":"day1/30_dockerMachine/#docker-machine","text":"Docker Machine is a tool for provisioning and managing your Dockerized hosts (hosts with Docker Engine on them) The Dockerized hosts themselves can be thought of, and are sometimes referred to as, managed \u201cmachines\u201d.","title":"Docker machine"},{"location":"day1/30_dockerMachine/#why-using-docker-machine","text":"We want an efficient way to provision machine with Docker pre-installed We want a small (in size) Docker enabled OS (Ubuntu 12.04.x is the default)","title":"Why using Docker Machine ?"},{"location":"day1/30_dockerMachine/#installation","text":"Follow: Docker machine installation link $ if [[ ! -d \" $HOME /bin\" ]] ; then mkdir -p \" $HOME /bin\" ; fi && \\ curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-Windows-x86_64.exe > \" $HOME /bin/docker-machine.exe\" && \\ chmod +x \" $HOME /bin/docker-machine.exe\" Important To play the above script, you will need git-bash","title":"Installation"},{"location":"day1/30_dockerMachine/#why-rancheros","text":"Several reasons : This is because Rancher is a cool company delivering KISS 1 cloud tools For Kubernetes, we will see 2 good On Premise 2 Rancher tools K3S a very light kubernetes dedicated to Edge 3 Computing It really nice as you can start really easily a Kubernetes cluster on Raspberry PI RKE (Rancher Kubernetes Engine) the professional Kubernetes packaging easy to use Note Another very cool company you should have a look to: Hashicorp Example of nice products: vagrant, packer, terraform, vault","title":"Why RancherOS ?"},{"location":"day1/30_dockerMachine/#links","text":"RancherOS release Rancher OS installation with dockerMachine Rancher OS overview","title":"Links"},{"location":"day1/30_dockerMachine/#labs-installing-rancheros-machines","text":"They are several objectives behind this LAB * Show you another virtual machine deployments * Explore and understand another modern packaging * Have machines ready to test docker * Have machines ready for a future Kubernetes cluster","title":"LABS: Installing RancherOS machines"},{"location":"day1/30_dockerMachine/#instruction","text":"You need to install 2 machines named \"ROSServer\" and \"ROSWorker1\"","title":"Instruction"},{"location":"day1/30_dockerMachine/#deployment-instructions","text":"To deploy a RancherOS machine, you will need this command. (you will have to adapt it) $ docker-machine create -d virtualbox \\ --virtualbox-boot2docker-url https://releases.rancher.com/os/latest/rancheros.iso \\ --virtualbox-memory <MEMORY-SIZE> \\ <MACHINE-NAME> Warning Use Git Bash to execute the following commands ( MobaXterm won't work well here) To check the VM are deployed: $ VBoxManage list runningvms | grep <MACHINE-NAME>","title":"Deployment instructions"},{"location":"day1/30_dockerMachine/#understand-the-installation","text":"Your turn Ask you some questions about the driver interface created ? What are the interface type ? Could you check your networks possibility ? Pinging machine (from your localhost, between the machines) Access to internet ? Tip To start the VirtualBox guest addition: ros service enable virtual-box ros service start virtual-box You need to reboot the VM to really activate the service","title":"Understand the installation"},{"location":"day1/30_dockerMachine/#rancheros-commands-summary","text":"COMMAND DESCRIPTION docker Good old Docker, use that to run stuff. system-docker The Docker instance running the system containers. (root rights) ros Control and configure RancherOS","title":"RancherOS commands summary"},{"location":"day1/30_dockerMachine/#labs-machine-setup","text":"","title":"LABS: machine setup"},{"location":"day1/30_dockerMachine/#changing-console","text":"The machine is particular there is by default no persistence on the OS console You can see the available console with: ros console list Change to ubuntu: ros console switch ubuntu You will have to restart the session Now you will get data persistence in your home like a traditional OS","title":"Changing console"},{"location":"day1/30_dockerMachine/#adding-git-for-future-labs","text":"We will need to have \"git\" on the machine # Add this alias to a ~/.bash_profile alias git = 'docker run -ti --rm -v /home/docker:/git bwits/docker-git-alpine' Note As you can see everything is docker ... !","title":"Adding git for future labs"},{"location":"day1/30_dockerMachine/#adding-docker-compose","text":"Before switching to kubernetes we would like to deploy complex things with docker. Let prepare the installation of docker-compose sudo wget https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m) -O /usr/bin/docker-compose sudo chmod +x /usr/bin/docker-compose","title":"Adding docker compose"},{"location":"day1/30_dockerMachine/#adding-your-own-private-key","text":"Use ssh-keygen command to add a private/public rsa key - so you can easily connect to your machine. (from MobaXterm for example) Tip Host rosserver HostName 127.0.0.1 User docker Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile ~/.ssh/id_ros_rsa IdentitiesOnly yes LogLevel FATAL Host rosworker1 HostName 127.0.0.1 User docker Port 2223 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile ~/.ssh/id_ros_rsa IdentitiesOnly yes Keep It Simple and Stupid \u21a9 \"On Premise\" means \"private\", we can say also \"In House\" \u21a9 Edge computing could be seen as a micro Data center (IOT / 5G) \u21a9","title":"Adding your own private key"},{"location":"day1/32_dockerBasics/","text":"Docker basics and a bit more Virtualbox required under Windows7 not under windows10 if you use the native virtualization Installation Depends on the OS. Windows: https://docs.docker.com/docker-for-windows/ Mac: https://docs.docker.com/docker-for-mac/ Linux : https://docs.docker.com/engine/install/ Tips If under Linux you don't have the right to execute Docker under your user account In that case, you need this: sudo usermod -aG docker $( whoami ) When docker is installed, a good starting point is this command: docker info This give you details around version and embedded plugins Image and runtime Dockerfile FROM ubuntu RUN apt-get update && apt-get install -y nginx COPY index.html /var/share/nginx/html CMD nginx -g \"daemon off\"; Network There are several network drivers : bridge : This is the default non routable network - adress 172.17.0.2. Used for standalones application host : The container is using the host network overlay : Virtual Embedded network added to the host network allowing containers communication (used in Kubernetes) None: Deactivated network Docker Network LABS List the networks on your machine docker network ls What is the default network ? One method: To see the default network, type a command like this one : docker run -ti nginx bash -c \"hostname -i\" docker run -d --name=nginx nginx:alpine Then you can inspect it, looking at the Networks entries docker inspect nginx Launch another container docker run --rm -it --name=alpine alpine check the network ping the nginx container To call remotly Nginx: wget -O- 172.17.0.2 Test internet access ? wget -O- wttr.in/Moon Try a container with no network interface docker run --rm -it --name=alpine_none --net=none alpine Could you check it has no network connectivity ? Container with Host network docker run --rm -it --name=alpine_host --net=host alpine Could you check it has access to host network ? Why using Host network ? Typical use case is a Jenkins CI that must access to host network resources ... Under the docker container: docker run --rm -it --name=alpine_host --net=host alpine { echo -e 'HTTP/1 200 OK\\r\\n'; echo \"Marvelous !\"; } | nc -v -l -p 80 Outside on the Host: wget -O- localhost Isolating docker networks by creating another one docker network create another_net docker run --rm -it --name=alpine_custom --net=another_net alpine Could you check it's really isolated ? Externet access We clean everything docker rm -f $(docker ps -qa) We will use PAT (Port Adress Translation) docker run -d --name=nginx --publish=80 nginx:alpine What is the port ? How do i get it ? check by calling nginx ... Luckily you can choose the mapping port docker run -d --name=nginx --publish=8080:80 nginx:alpine to go further: docker swarm This Url allows to play with swarm with play with docker TODO: give details here Data Storage The most common storage drivers are AUFS, Overlay/Overlay2, Devicemapper, Btrfs, and ZFS. All storage drivers can be categorized into three different types: Storage driver category Storage drivers Union filesystems AUFS, Overlay, Overlay2 Snapshotting filesystems Btrfs, ZFS Copy-on-write block devices Devicemapper Bind mounts A simple and traditional way for data sharing between a host and guest : docker run --rm -ti --volume /home/ubuntu/foo:/foo alpine sh Warning Security leak as we are accessing the host volume Not portable Better option, binding mounts using Volumes docker run -d --name foo -ti --volume /foo alpine sh docker run -d --name bar -ti --volumes-from foo alpine sh We can see the volume with these commands : docker info|grep \"Docker Root Dir\" Docker Root Dir: /var/snap/docker/common/var-lib-docker sudo ls -als /var/snap/docker/common/var-lib-docker docker volume ls Public registry Docker Hub is the default public registry. There are other public registry free for reading image (Redhat http://quay.io for example) Note Don't hesitate to create your own free registry at Docker Hub. Private registry Warning Don't confuse between Saas private registry (so managed registry) and company private registry. Even at Docker you can have private registry (one free per account) Private registries are used to manage your company artifacts. Some common private registry : Sonatype Nexus Jfrog Artifactory To use a private registry you need to deploy some private certificate on your Docker client. Some commands may be restricted - it depends on the server configuration ( \"docker search\" forbidden for instance) When you are in a corporate company, your Docker build may rely on remote internet registries you will need a proxy to build new images. Several options in front of you: Transparent proxy usage Add proxy options to your build like --build-args PROXY=proxy.fr.myCompany:8080 Proxy in the daemon docker configuration Or (best option) your private registry is doing the proxy for you Todo Add a schema !!! Build image - Slimming stage Code to deploy helloworld.go : 1 2 3 4 5 package main import \"fmt\" func main () { fmt . Println ( \"Hello, world!\" ) } Build : env GOOS=linux GOARCH=amd64 go build -o helloworld . Important you need to install golang before # on ubuntu sudo apt install golang-go Beginner Dockerfile: FROM ubuntu:xenial COPY hello hello ENTRYPOINT [\"./hello\"] What is the issue there ? Intermediary level Dockerfile: FROM alpine:3.8 COPY hello hello ENTRYPOINT [\"./hello\"] Much better ... Seasoned integrator Dockerfile using \u201cstage\u201d builds # Step 1 FROM golang:alpine3.7 as build-env COPY main.go hello/main.go WORKDIR hello RUN env GOOS = linux GOARCH = amd64 go build # Step 2 FROM scratch COPY --from = build-env /go/hello/hello /go/bin/hello ENTRYPOINT [ \"/go/bin/hello\" ] My Dockerfile becomes includes a Continuous Integration tool stage as the build part is included ... and the size is reduced to the max ! For fun, ask dockerfile for a Go helloworld to chatGPT ChatGPT","title":"Docker - Basics"},{"location":"day1/32_dockerBasics/#docker-basics-and-a-bit-more","text":"Virtualbox required under Windows7 not under windows10 if you use the native virtualization","title":"Docker basics and a bit more"},{"location":"day1/32_dockerBasics/#installation","text":"Depends on the OS. Windows: https://docs.docker.com/docker-for-windows/ Mac: https://docs.docker.com/docker-for-mac/ Linux : https://docs.docker.com/engine/install/ Tips If under Linux you don't have the right to execute Docker under your user account In that case, you need this: sudo usermod -aG docker $( whoami ) When docker is installed, a good starting point is this command: docker info This give you details around version and embedded plugins","title":"Installation"},{"location":"day1/32_dockerBasics/#image-and-runtime","text":"Dockerfile FROM ubuntu RUN apt-get update && apt-get install -y nginx COPY index.html /var/share/nginx/html CMD nginx -g \"daemon off\";","title":"Image and runtime"},{"location":"day1/32_dockerBasics/#network","text":"There are several network drivers : bridge : This is the default non routable network - adress 172.17.0.2. Used for standalones application host : The container is using the host network overlay : Virtual Embedded network added to the host network allowing containers communication (used in Kubernetes) None: Deactivated network","title":"Network"},{"location":"day1/32_dockerBasics/#docker-network-labs","text":"List the networks on your machine docker network ls What is the default network ? One method: To see the default network, type a command like this one : docker run -ti nginx bash -c \"hostname -i\" docker run -d --name=nginx nginx:alpine Then you can inspect it, looking at the Networks entries docker inspect nginx Launch another container docker run --rm -it --name=alpine alpine check the network ping the nginx container To call remotly Nginx: wget -O- 172.17.0.2 Test internet access ? wget -O- wttr.in/Moon Try a container with no network interface docker run --rm -it --name=alpine_none --net=none alpine Could you check it has no network connectivity ? Container with Host network docker run --rm -it --name=alpine_host --net=host alpine Could you check it has access to host network ? Why using Host network ? Typical use case is a Jenkins CI that must access to host network resources ... Under the docker container: docker run --rm -it --name=alpine_host --net=host alpine { echo -e 'HTTP/1 200 OK\\r\\n'; echo \"Marvelous !\"; } | nc -v -l -p 80 Outside on the Host: wget -O- localhost Isolating docker networks by creating another one docker network create another_net docker run --rm -it --name=alpine_custom --net=another_net alpine Could you check it's really isolated ? Externet access We clean everything docker rm -f $(docker ps -qa) We will use PAT (Port Adress Translation) docker run -d --name=nginx --publish=80 nginx:alpine What is the port ? How do i get it ? check by calling nginx ... Luckily you can choose the mapping port docker run -d --name=nginx --publish=8080:80 nginx:alpine","title":"Docker Network LABS"},{"location":"day1/32_dockerBasics/#to-go-further-docker-swarm","text":"This Url allows to play with swarm with play with docker TODO: give details here","title":"to go further: docker swarm"},{"location":"day1/32_dockerBasics/#data-storage","text":"The most common storage drivers are AUFS, Overlay/Overlay2, Devicemapper, Btrfs, and ZFS. All storage drivers can be categorized into three different types: Storage driver category Storage drivers Union filesystems AUFS, Overlay, Overlay2 Snapshotting filesystems Btrfs, ZFS Copy-on-write block devices Devicemapper","title":"Data Storage"},{"location":"day1/32_dockerBasics/#bind-mounts","text":"A simple and traditional way for data sharing between a host and guest : docker run --rm -ti --volume /home/ubuntu/foo:/foo alpine sh Warning Security leak as we are accessing the host volume Not portable Better option, binding mounts using Volumes docker run -d --name foo -ti --volume /foo alpine sh docker run -d --name bar -ti --volumes-from foo alpine sh We can see the volume with these commands : docker info|grep \"Docker Root Dir\" Docker Root Dir: /var/snap/docker/common/var-lib-docker sudo ls -als /var/snap/docker/common/var-lib-docker docker volume ls","title":"Bind mounts"},{"location":"day1/32_dockerBasics/#public-registry","text":"Docker Hub is the default public registry. There are other public registry free for reading image (Redhat http://quay.io for example) Note Don't hesitate to create your own free registry at Docker Hub.","title":"Public registry"},{"location":"day1/32_dockerBasics/#private-registry","text":"Warning Don't confuse between Saas private registry (so managed registry) and company private registry. Even at Docker you can have private registry (one free per account) Private registries are used to manage your company artifacts. Some common private registry : Sonatype Nexus Jfrog Artifactory To use a private registry you need to deploy some private certificate on your Docker client. Some commands may be restricted - it depends on the server configuration ( \"docker search\" forbidden for instance) When you are in a corporate company, your Docker build may rely on remote internet registries you will need a proxy to build new images. Several options in front of you: Transparent proxy usage Add proxy options to your build like --build-args PROXY=proxy.fr.myCompany:8080 Proxy in the daemon docker configuration Or (best option) your private registry is doing the proxy for you Todo Add a schema !!!","title":"Private registry"},{"location":"day1/32_dockerBasics/#build-image-slimming-stage","text":"","title":"Build image - Slimming stage"},{"location":"day1/32_dockerBasics/#code-to-deploy","text":"helloworld.go : 1 2 3 4 5 package main import \"fmt\" func main () { fmt . Println ( \"Hello, world!\" ) } Build : env GOOS=linux GOARCH=amd64 go build -o helloworld . Important you need to install golang before # on ubuntu sudo apt install golang-go","title":"Code to deploy"},{"location":"day1/32_dockerBasics/#beginner","text":"Dockerfile: FROM ubuntu:xenial COPY hello hello ENTRYPOINT [\"./hello\"] What is the issue there ?","title":"Beginner"},{"location":"day1/32_dockerBasics/#intermediary-level","text":"Dockerfile: FROM alpine:3.8 COPY hello hello ENTRYPOINT [\"./hello\"] Much better ...","title":"Intermediary level"},{"location":"day1/32_dockerBasics/#seasoned-integrator","text":"Dockerfile using \u201cstage\u201d builds # Step 1 FROM golang:alpine3.7 as build-env COPY main.go hello/main.go WORKDIR hello RUN env GOOS = linux GOARCH = amd64 go build # Step 2 FROM scratch COPY --from = build-env /go/hello/hello /go/bin/hello ENTRYPOINT [ \"/go/bin/hello\" ] My Dockerfile becomes includes a Continuous Integration tool stage as the build part is included ... and the size is reduced to the max !","title":"Seasoned integrator"},{"location":"day1/32_dockerBasics/#for-fun-ask-dockerfile-for-a-go-helloworld-to-chatgpt","text":"ChatGPT","title":"For fun, ask dockerfile for a Go helloworld to chatGPT"},{"location":"day1/34_dockerCompose/","text":"Docker compose Compose is a tool for defining and running multi-container Docker applications . With Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration Links Overview of Docker compose Docker compose release Install docker-compose executable. Then install the docker-compose.yaml file to execute. Labs 1: Wordpress Please follow the \"Compose and Wordpress\" quickstart. wordpress url 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 version : \"3.9\" services : db : image : mysql:5.7 volumes : - db_data:/var/lib/mysql restart : always environment : MYSQL_ROOT_PASSWORD : somewordpress MYSQL_DATABASE : wordpress MYSQL_USER : wordpress MYSQL_PASSWORD : wordpress wordpress : depends_on : - db image : wordpress:latest volumes : - wordpress_data:/var/www/html ports : - \"8000:80\" restart : always environment : WORDPRESS_DB_HOST : db WORDPRESS_DB_USER : wordpress WORDPRESS_DB_PASSWORD : wordpress WORDPRESS_DB_NAME : wordpress volumes : db_data : {} wordpress_data : {} Labs 2: A complete monitoring infrastructure We will deploy this : The project URL is this one : dockprom","title":"Docker compose"},{"location":"day1/34_dockerCompose/#docker-compose","text":"Compose is a tool for defining and running multi-container Docker applications . With Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration","title":"Docker compose"},{"location":"day1/34_dockerCompose/#links","text":"Overview of Docker compose Docker compose release Install docker-compose executable. Then install the docker-compose.yaml file to execute.","title":"Links"},{"location":"day1/34_dockerCompose/#labs-1-wordpress","text":"Please follow the \"Compose and Wordpress\" quickstart. wordpress url 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 version : \"3.9\" services : db : image : mysql:5.7 volumes : - db_data:/var/lib/mysql restart : always environment : MYSQL_ROOT_PASSWORD : somewordpress MYSQL_DATABASE : wordpress MYSQL_USER : wordpress MYSQL_PASSWORD : wordpress wordpress : depends_on : - db image : wordpress:latest volumes : - wordpress_data:/var/www/html ports : - \"8000:80\" restart : always environment : WORDPRESS_DB_HOST : db WORDPRESS_DB_USER : wordpress WORDPRESS_DB_PASSWORD : wordpress WORDPRESS_DB_NAME : wordpress volumes : db_data : {} wordpress_data : {}","title":"Labs 1: Wordpress"},{"location":"day1/34_dockerCompose/#labs-2-a-complete-monitoring-infrastructure","text":"We will deploy this : The project URL is this one : dockprom","title":"Labs 2: A complete monitoring infrastructure"},{"location":"day2/cloudIntroduction/","text":"An introduction to Cloud This is more an awareness and vocabulary presentation. What is Cloud Computing Different type of Cloud computing : Services in the Cloud On demand self-service Broad network access Resource pooling Rapid elasticity Measured service Cloud offer and service provided Different Cloud deployment The need to have common Cloud service deployment in both Public and Private Cloud explain the rise of Kubernetes. Kubernetes address one crucial risk for companies: lock-in. Having the same deployment infrastructure target, the companies can move from Public to Private Cloud or create Hybrid CLoud. Quizz Question Which cloud delivery type below abstracts runtime infrastructure, middleware and operating system in addition to compute, network and storage resources? IaaS PaaS SaaS Question Which one of the following is not an essential characteristic of cloud computing? On demand self-service Rapid elasticity Measured service Open source Resource pooling Broad network access Quick openstack presentation What is Openstack ? Open source software for creating private and public clouds (OVH for example) Compute, storage, and networking Huge demand for expertise Success OpenStack is one of the top 3 most active open source projects (Python) and manages 15 million compute cores You can have a better idea of project size looking at: Openstack User Stories OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, + all managed and provisioned through APIs with common authentication mechanisms. A dashboard is also available, giving administrators control while empowering their users to provision resources through a web interface. + Beyond standard infrastructure-as-a-service functionality, additional components provide orchestration, fault management and service management amongst other services to ensure high availability of user applications. + Openstack Hypervisors OpenStack Compute (nova) supports many hypervisors to various degrees, including: KVM LXC QEMU VMware ESX/ESXi Xen Hyper-V Docker Openstack full picture Give a better idea of the project size : Is OpenStack Dead 2020? Yes, OpenStack is still alive and well, and it continues to evolve with interesting new features, like StarlingX . But many of the vendors that originally specialized in OpenStack, such as Mirantis, have now shifted their attention to Kubernetes. Others, like SUSE, have dropped out of the OpenStack market altogether. SUSE bought Rancher in mid 2020.","title":"Cloud - Introduction to Cloud"},{"location":"day2/cloudIntroduction/#an-introduction-to-cloud","text":"This is more an awareness and vocabulary presentation.","title":"An introduction to Cloud"},{"location":"day2/cloudIntroduction/#what-is-cloud-computing","text":"Different type of Cloud computing :","title":"What is Cloud Computing"},{"location":"day2/cloudIntroduction/#services-in-the-cloud","text":"On demand self-service Broad network access Resource pooling Rapid elasticity Measured service","title":"Services in the Cloud"},{"location":"day2/cloudIntroduction/#cloud-offer-and-service-provided","text":"","title":"Cloud offer and service provided"},{"location":"day2/cloudIntroduction/#different-cloud-deployment","text":"The need to have common Cloud service deployment in both Public and Private Cloud explain the rise of Kubernetes. Kubernetes address one crucial risk for companies: lock-in. Having the same deployment infrastructure target, the companies can move from Public to Private Cloud or create Hybrid CLoud.","title":"Different Cloud deployment"},{"location":"day2/cloudIntroduction/#quizz","text":"Question Which cloud delivery type below abstracts runtime infrastructure, middleware and operating system in addition to compute, network and storage resources? IaaS PaaS SaaS Question Which one of the following is not an essential characteristic of cloud computing? On demand self-service Rapid elasticity Measured service Open source Resource pooling Broad network access","title":"Quizz"},{"location":"day2/cloudIntroduction/#quick-openstack-presentation","text":"","title":"Quick openstack presentation"},{"location":"day2/cloudIntroduction/#what-is-openstack","text":"Open source software for creating private and public clouds (OVH for example) Compute, storage, and networking Huge demand for expertise Success OpenStack is one of the top 3 most active open source projects (Python) and manages 15 million compute cores You can have a better idea of project size looking at: Openstack User Stories OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, + all managed and provisioned through APIs with common authentication mechanisms. A dashboard is also available, giving administrators control while empowering their users to provision resources through a web interface. + Beyond standard infrastructure-as-a-service functionality, additional components provide orchestration, fault management and service management amongst other services to ensure high availability of user applications. +","title":"What is Openstack ?"},{"location":"day2/cloudIntroduction/#openstack-hypervisors","text":"OpenStack Compute (nova) supports many hypervisors to various degrees, including: KVM LXC QEMU VMware ESX/ESXi Xen Hyper-V Docker","title":"Openstack Hypervisors"},{"location":"day2/cloudIntroduction/#openstack-full-picture","text":"Give a better idea of the project size :","title":"Openstack full picture"},{"location":"day2/cloudIntroduction/#is-openstack-dead-2020","text":"Yes, OpenStack is still alive and well, and it continues to evolve with interesting new features, like StarlingX . But many of the vendors that originally specialized in OpenStack, such as Mirantis, have now shifted their attention to Kubernetes. Others, like SUSE, have dropped out of the OpenStack market altogether. SUSE bought Rancher in mid 2020.","title":"Is OpenStack Dead 2020?"},{"location":"day2/day1Summary/","text":"Quick Day1 summary Vagrant - multi machine using Debian buster64 Link : https://github.com/omerlin/yncrea-virtualization-labs/tree/main/vagrant/buster64 Schema: Note Drawing done with drawio tool Portable version here: drawio portable version The above PNG file example: Virtualbox network with 4 Debian machines - Drawio LABS Restart the buster64 labs using vagrant Reminder on VBOXmanager: vboxmanage command (See cheatsheet VBOXManage ) Snapshot of machine Restore of machine snapshot LABS : update your VM Update the Debian OS ( apt-get update ) Stop the VMs Take a snapshot of the all your boxes Reminder on MobaXterm installation Get the portable version, install it and define a location for home directory (in settings/General) Mobaxterm aliases reminder You can connect using 3 ways: * using the GUI * using vagrant command vagrant ssh * using VboxManage startvm Question VBoxManage is the best option - tell me why ? :Like: Tip To start all the VM from a bash script (mobaXterm for instance) # bash way to start all the Vms for k in ` vboxmanage list vms | cut -d '\"' -f 2 ` ; do vboxmanage startvm $k --type headless ; done # Same to stop all the Vms cleanly for k in ` vboxmanage list runningvms | cut -d '\"' -f 2 ` ; do vboxmanage controlvm $k poweroff soft ; done Reminder # powershell vboxmanage showvminfo controller - -details | select-string 'Nic 1 rule' or in bash, to get only port: # bash vboxmanage showvminfo controller --details | grep 'NIC 1 Rule' | cut -d ',' -f 4 | cut -d '=' -f 2 | xargs # all the boxes port for k in ` vboxmanage list vms | cut -d '\"' -f 2 ` ; do port = $( vboxmanage showvminfo $k --details | grep 'NIC 1 Rule' | cut -d ',' -f 4 | cut -d '=' -f 2 | xargs ) ; echo \" $k -- $port \" ; done File ~/.ssh/config to define a SSH alias on Linux. # This path depends n your installation cd $( cygpath $USERPROFILE ) /MyApp/yncrea-virtualization-labs/vagrant/buster64 # Get private keys and copy them in $HOME/.ssh directory for k in ` find . -name \"private*\" ` ; do t = $( echo $k | cut -d '/' -f 4 ) ; cp -f $k ~/.ssh/id_ $t ; done Then you need to configure aliases in the ~/.ssh/config file UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentitiesOnly yes LogLevel INFO Host controller c1 HostName 127.0.0.1 User vagrant Port 2202 IdentityFile ~/.ssh/id_controller LocalForward 8000 localhost:8000 LAB Do the configuration and test it Alternative: Host-only network One may advantage: you can interract directly with the Host and thus be reachable from WSL 2 VM. As WSL2 VM are very light this is a great advantage. To configure it it's easy. You need a Host network. This can be created with the GUI or : vboxmanage hostonlyif create vboxmanage list hostonlyifs then you re-create your vagrant image (or update - but not tested) so to use Vagrant . configure ( 2 ) do | config | config . vm . define \"controller\" do | worker | worker . vm . allow_hosts_modification = true worker . vm . hostname = \"controller\" worker . vm . box = \"bionic64\" worker . vm . network \"private_network\" , ip : \"192.168.83.10\" , name : \"VirtualBox Host-Only Ethernet Adapter\" worker . vm . provision \"shell\" , path : \"../scripts/install.sh\" worker . vm . provider \"virtualbox\" do | v | v . name = \"controller\" v . memory = 1024 v . cpus = 1 end end end Docker Basic of Docker & Docker slimming Just a reminder of basics Reminds also about the docker network labs LABS build and execute a small nodeJS application Start worker1 for instance You need to clone the link in one VM git clone https://github.com/omerlin/yncrea-virtualization-labs.git cd yncrea-virtualization-labs Then you have to: build the docker image start the image (don't forget to expose the port) test the image from a browser Pushing image to a repository Warning You need to have an account on https://hub.docker.com docker images # You get the image id you built locally docker tag <<image_i d>>>> omerlin/nodeapp:v01 docker login -u omerlin docker push omerlin/nodeapp:v01 Docker compose We will redo a quick labs on it Still on worker1, go to the yncrea-virtualization-labs git project Install docker-compose in your Linux box (worker1 for instance), using: sudo apt install docker-compose Now, we will update the small nodejs application, to persist data in a Mysql database. Look carefully at the docker-compose.yml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version : \"3\" services : web : build : . # image: omerlin/node-app:1.0 command : node index-db.js ports : - \"3000:3000\" restart : on-failure environment : MYSQL_DATABASE : myapp MYSQL_USER : myapp MYSQL_PASSWORD : mysecurepass MYSQL_HOST : db db : image : woahbase/alpine-mysql:x86_64 expose : - \"3306\" environment : MYSQL_ROOT_PWD : insecurebydefault MYSQL_USER_DB : myapp MYSQL_USER : myapp MYSQL_USER_PWD : mysecurepass Line Description line 4 it's better to put the build image name line 9 without this restart: on-failure , it fails as the Node program start before the database up line 14 this refers the db container line 17 we expose the port for other pods in the overlay network Note You may have remarked all the images are Alpine based images LAB Make it working ... There is a small trap","title":"Day1 - quick summary"},{"location":"day2/day1Summary/#quick-day1-summary","text":"","title":"Quick Day1 summary"},{"location":"day2/day1Summary/#vagrant-multi-machine-using-debian-buster64","text":"Link : https://github.com/omerlin/yncrea-virtualization-labs/tree/main/vagrant/buster64 Schema: Note Drawing done with drawio tool Portable version here: drawio portable version The above PNG file example: Virtualbox network with 4 Debian machines - Drawio LABS Restart the buster64 labs using vagrant","title":"Vagrant - multi machine using Debian buster64"},{"location":"day2/day1Summary/#reminder-on-vboxmanager","text":"vboxmanage command (See cheatsheet VBOXManage ) Snapshot of machine Restore of machine snapshot LABS : update your VM Update the Debian OS ( apt-get update ) Stop the VMs Take a snapshot of the all your boxes","title":"Reminder on VBOXmanager:"},{"location":"day2/day1Summary/#reminder-on-mobaxterm-installation","text":"Get the portable version, install it and define a location for home directory (in settings/General)","title":"Reminder on MobaXterm installation"},{"location":"day2/day1Summary/#mobaxterm-aliases-reminder","text":"You can connect using 3 ways: * using the GUI * using vagrant command vagrant ssh * using VboxManage startvm Question VBoxManage is the best option - tell me why ? :Like: Tip To start all the VM from a bash script (mobaXterm for instance) # bash way to start all the Vms for k in ` vboxmanage list vms | cut -d '\"' -f 2 ` ; do vboxmanage startvm $k --type headless ; done # Same to stop all the Vms cleanly for k in ` vboxmanage list runningvms | cut -d '\"' -f 2 ` ; do vboxmanage controlvm $k poweroff soft ; done Reminder # powershell vboxmanage showvminfo controller - -details | select-string 'Nic 1 rule' or in bash, to get only port: # bash vboxmanage showvminfo controller --details | grep 'NIC 1 Rule' | cut -d ',' -f 4 | cut -d '=' -f 2 | xargs # all the boxes port for k in ` vboxmanage list vms | cut -d '\"' -f 2 ` ; do port = $( vboxmanage showvminfo $k --details | grep 'NIC 1 Rule' | cut -d ',' -f 4 | cut -d '=' -f 2 | xargs ) ; echo \" $k -- $port \" ; done File ~/.ssh/config to define a SSH alias on Linux. # This path depends n your installation cd $( cygpath $USERPROFILE ) /MyApp/yncrea-virtualization-labs/vagrant/buster64 # Get private keys and copy them in $HOME/.ssh directory for k in ` find . -name \"private*\" ` ; do t = $( echo $k | cut -d '/' -f 4 ) ; cp -f $k ~/.ssh/id_ $t ; done Then you need to configure aliases in the ~/.ssh/config file UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentitiesOnly yes LogLevel INFO Host controller c1 HostName 127.0.0.1 User vagrant Port 2202 IdentityFile ~/.ssh/id_controller LocalForward 8000 localhost:8000 LAB Do the configuration and test it","title":"Mobaxterm aliases reminder"},{"location":"day2/day1Summary/#alternative-host-only-network","text":"One may advantage: you can interract directly with the Host and thus be reachable from WSL 2 VM. As WSL2 VM are very light this is a great advantage. To configure it it's easy. You need a Host network. This can be created with the GUI or : vboxmanage hostonlyif create vboxmanage list hostonlyifs then you re-create your vagrant image (or update - but not tested) so to use Vagrant . configure ( 2 ) do | config | config . vm . define \"controller\" do | worker | worker . vm . allow_hosts_modification = true worker . vm . hostname = \"controller\" worker . vm . box = \"bionic64\" worker . vm . network \"private_network\" , ip : \"192.168.83.10\" , name : \"VirtualBox Host-Only Ethernet Adapter\" worker . vm . provision \"shell\" , path : \"../scripts/install.sh\" worker . vm . provider \"virtualbox\" do | v | v . name = \"controller\" v . memory = 1024 v . cpus = 1 end end end","title":"Alternative: Host-only network"},{"location":"day2/day1Summary/#docker","text":"","title":"Docker"},{"location":"day2/day1Summary/#basic-of-docker-docker-slimming","text":"Just a reminder of basics Reminds also about the docker network labs","title":"Basic of Docker &amp; Docker slimming"},{"location":"day2/day1Summary/#labs-build-and-execute-a-small-nodejs-application","text":"Start worker1 for instance You need to clone the link in one VM git clone https://github.com/omerlin/yncrea-virtualization-labs.git cd yncrea-virtualization-labs Then you have to: build the docker image start the image (don't forget to expose the port) test the image from a browser","title":"LABS build and execute a small nodeJS application"},{"location":"day2/day1Summary/#pushing-image-to-a-repository","text":"Warning You need to have an account on https://hub.docker.com docker images # You get the image id you built locally docker tag <<image_i d>>>> omerlin/nodeapp:v01 docker login -u omerlin docker push omerlin/nodeapp:v01","title":"Pushing image to a repository"},{"location":"day2/day1Summary/#docker-compose","text":"We will redo a quick labs on it Still on worker1, go to the yncrea-virtualization-labs git project Install docker-compose in your Linux box (worker1 for instance), using: sudo apt install docker-compose Now, we will update the small nodejs application, to persist data in a Mysql database. Look carefully at the docker-compose.yml file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version : \"3\" services : web : build : . # image: omerlin/node-app:1.0 command : node index-db.js ports : - \"3000:3000\" restart : on-failure environment : MYSQL_DATABASE : myapp MYSQL_USER : myapp MYSQL_PASSWORD : mysecurepass MYSQL_HOST : db db : image : woahbase/alpine-mysql:x86_64 expose : - \"3306\" environment : MYSQL_ROOT_PWD : insecurebydefault MYSQL_USER_DB : myapp MYSQL_USER : myapp MYSQL_USER_PWD : mysecurepass Line Description line 4 it's better to put the build image name line 9 without this restart: on-failure , it fails as the Node program start before the database up line 14 this refers the db container line 17 we expose the port for other pods in the overlay network Note You may have remarked all the images are Alpine based images LAB Make it working ... There is a small trap","title":"Docker compose"},{"location":"day2/kubernetesBasics/","text":"Kubernetes basics Global architecture Worker node architecture Replication controller & replication set Stateless Vs Stateful If your application is stateless , you can horizontally scale it Staleless = your application don't have a state, it doesn't write any local files/ keep local session Stateful All traditional database (PostgreSQL, MySQL) are stateful , they have database files that can't be split over multiple instance Note This is a big difference with NoSQL database, like Cassandra, where scalability is possible - but the database is no more ACID See also the [CAP theorem]'( https://en.wikipedia.org/wiki/CAP_theorem ) Most Web application can be made stateless: Session management must be done outside the container Any files that need to be saved can't be saved locally on the container Scaling in Kubernetes Scaling in Kubernetes can be done using the -- Replication Controller -- / Replication Set Note Replication Set is now replacing Replication Controller has this is able to do filtering based on a set of values whereas the Replication Controller was able only to use equality (like \"card\" == \"GPU\") Replication Set are used in background by deployment A \"replication set\" will ensure a number of pod replicas will run at all time A pod stared with the replica set (replica controller) will automatically be replaced if they fail, get deleted or are terminated. Using a replicat set is also recommended if you just want to ensure 1 pod is always running , even after reboot in that case, you configure with replica=1 You are sure that the pod is always running Deployment Deployment declaration in Kubernetes allows you to do deployments and updates When doing a deployment, you define the state of your application then Kubernetes will ensure the cluster will match your desired state This would be difficult to achieve with Replication Controller / Replication Set So the Deployment Object is easier to use and givers more possibilities apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-deployment spec : replicas : 3 selector : matchLabels : app : helloworld template : metadata : labels : app : helloworld spec : containers : - name : k8s-demo image : omerlin/nodeapp:v01 ports : - name : nodejs-port containerPort : 3000 Useful commands LAB deployment We will use K3s for this lab please follow the instruction on the Kubernetes installation page git clone https://github.com/omerlin/yncrea-virtualization-labs.git * First, test and validate the deployment Reminder you need to use the kubectl create -f ... command Second, you will create a new deployment name helloworld-nginx.yml using this image: karthequian/helloworld:latest Change the deployment name to helloworld-nginx-deployment Now check Deploy it with the \u201ckubectl create\u201d command Look at the deployment resource Look at the replicasets behind Look at the pods showing the labels Now have a look at : https://hub.docker.com/r/karthequian/helloworld/ Check for the different Tags You will play with rolling update But before, we need to expose the service kubectl expose deployment helloworld-nginx-deployment --type=NodePort You can look at the node port with the command: kubectl get service More details with : kubectl describe deployment \u2026 on minikube To get the url: minikube service helloworld-nginx-deployment --url Look at the page with your browser Tip Don't forget to forward the port Ok we will switch to another image tag: blue kubectl set image deployment helloworld-nginx-deployment k8s-demo=karthequian/helloworld:blue Check the rollout kubectl rollout status deployment/helloworld-nginx-deployment Check with your browser the image has been updated No look at the rollout history Do an undo & check it has worked \u2026 Look at it age with the kubectl get pods RevisionHistoryLimit is set to 10, you may change it with a kubectl edit command","title":"Kubernetes - basics"},{"location":"day2/kubernetesBasics/#kubernetes-basics","text":"","title":"Kubernetes basics"},{"location":"day2/kubernetesBasics/#global-architecture","text":"","title":"Global architecture"},{"location":"day2/kubernetesBasics/#worker-node-architecture","text":"","title":"Worker node architecture"},{"location":"day2/kubernetesBasics/#replication-controller-replication-set","text":"","title":"Replication controller &amp; replication set"},{"location":"day2/kubernetesBasics/#stateless-vs-stateful","text":"If your application is stateless , you can horizontally scale it Staleless = your application don't have a state, it doesn't write any local files/ keep local session Stateful All traditional database (PostgreSQL, MySQL) are stateful , they have database files that can't be split over multiple instance Note This is a big difference with NoSQL database, like Cassandra, where scalability is possible - but the database is no more ACID See also the [CAP theorem]'( https://en.wikipedia.org/wiki/CAP_theorem ) Most Web application can be made stateless: Session management must be done outside the container Any files that need to be saved can't be saved locally on the container","title":"Stateless Vs Stateful"},{"location":"day2/kubernetesBasics/#scaling-in-kubernetes","text":"Scaling in Kubernetes can be done using the -- Replication Controller -- / Replication Set Note Replication Set is now replacing Replication Controller has this is able to do filtering based on a set of values whereas the Replication Controller was able only to use equality (like \"card\" == \"GPU\") Replication Set are used in background by deployment A \"replication set\" will ensure a number of pod replicas will run at all time A pod stared with the replica set (replica controller) will automatically be replaced if they fail, get deleted or are terminated. Using a replicat set is also recommended if you just want to ensure 1 pod is always running , even after reboot in that case, you configure with replica=1 You are sure that the pod is always running","title":"Scaling in Kubernetes"},{"location":"day2/kubernetesBasics/#deployment","text":"Deployment declaration in Kubernetes allows you to do deployments and updates When doing a deployment, you define the state of your application then Kubernetes will ensure the cluster will match your desired state This would be difficult to achieve with Replication Controller / Replication Set So the Deployment Object is easier to use and givers more possibilities apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-deployment spec : replicas : 3 selector : matchLabels : app : helloworld template : metadata : labels : app : helloworld spec : containers : - name : k8s-demo image : omerlin/nodeapp:v01 ports : - name : nodejs-port containerPort : 3000","title":"Deployment"},{"location":"day2/kubernetesBasics/#useful-commands","text":"","title":"Useful commands"},{"location":"day2/kubernetesBasics/#lab-deployment","text":"We will use K3s for this lab please follow the instruction on the Kubernetes installation page git clone https://github.com/omerlin/yncrea-virtualization-labs.git * First, test and validate the deployment Reminder you need to use the kubectl create -f ... command Second, you will create a new deployment name helloworld-nginx.yml using this image: karthequian/helloworld:latest Change the deployment name to helloworld-nginx-deployment Now check Deploy it with the \u201ckubectl create\u201d command Look at the deployment resource Look at the replicasets behind Look at the pods showing the labels Now have a look at : https://hub.docker.com/r/karthequian/helloworld/ Check for the different Tags You will play with rolling update But before, we need to expose the service kubectl expose deployment helloworld-nginx-deployment --type=NodePort You can look at the node port with the command: kubectl get service More details with : kubectl describe deployment \u2026 on minikube To get the url: minikube service helloworld-nginx-deployment --url Look at the page with your browser Tip Don't forget to forward the port Ok we will switch to another image tag: blue kubectl set image deployment helloworld-nginx-deployment k8s-demo=karthequian/helloworld:blue Check the rollout kubectl rollout status deployment/helloworld-nginx-deployment Check with your browser the image has been updated No look at the rollout history Do an undo & check it has worked \u2026 Look at it age with the kubectl get pods RevisionHistoryLimit is set to 10, you may change it with a kubectl edit command","title":"LAB deployment"},{"location":"day2/kubernetesInstallation/","text":"Kubernetes installation Introduction Kubernetes can be installed in many ways. We will concentrate here on Private Home Office installation. As this is an incredibly changing world , we are testing some tools having in mind following KISS principle even for production ready cluster. Method Scope / Usage Alternative minikube Dev environment microk8s k3d or kubeadm k3s Edge computing, ARM microk8s also rke Production cluster kubeadm Warning After many test, it seems to me better and more reliable to rely on K3s for our hands-on minikube is also a good choice ... but experimented bad/strange behavior on my Windows 7 laptop Checking installation Link: This is how to check Kubernes is really working K3S from Rancher Quick Installation and architecture is here: Lightweight Kubernetes Reference documentation is here: Reference K3S documentation master k3s installation with vagrant video worker k3s installation with vagrant video K3S cluster local installation Installation on our Vagrant environment Note we increased the number of core=2 and memory=2048m on worker1 WE CAN STOP DOCKER SERVICE master installation VBoxManage snapshot worker1 restore snap-worker1-initial VBoxManage startvm worker1 --type headless VBoxManage startvm worker2 --type headless Then ssh worker1 : # root sudo -i root@box1:~$ curl -sfL https://get.k3s.io | sh - [INFO] Finding release for channel stable [INFO] Using v1.20.2+k3s1 as release [INFO] Downloading hash https://github.com/rancher/k3s/releases/download/v1.20.2+k3s1/sha256sum-amd64.txt [INFO] Downloading binary https://github.com/rancher/k3s/releases/download/v1.20.2+k3s1/k3s [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s vagrant@box1:~$ vagrant@box1:~$ vagrant@box1:~$ sudo k3s kubectl get node NAME STATUS ROLES AGE VERSION box1 Ready control-plane,master 55s v1.20.2+k3s1 Important The vagrant installation deploy the fixed NAT address on eth0 ... but the communication interface is eth1 (10.0.3.1/24) so we need to tell to the CNI that the real network is on this interface To do this, we have to update the k3s service /etc/systemd/system/k3s.service and add this : ExecStart = /usr/local/bin/k3s \\ server \\ --flannel-iface 'eth1' A script to play this patch: echo \" --flannel-iface 'eth1'\" >>/etc/systemd/system/k3s.service systemctl daemon-reload systemctl restart k3s To get rights as vagrant user: vagrant@box1:~$ export KUBECONFIG=\"/etc/rancher/k3s/k3s.yaml\" vagrant@box1:~$ sudo chmod o+r /etc/rancher/k3s/k3s.yaml vagrant@box1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system metrics-server-86cbb8457f-45xvh 1/1 Running 0 3m11s kube-system local-path-provisioner-7c458769fb-c7rbv 1/1 Running 0 3m11s kube-system coredns-854c77959c-mh47k 1/1 Running 0 3m11s kube-system helm-install-traefik-hpjb2 0/1 Completed 0 3m11s kube-system svclb-traefik-kt4k4 2/2 Running 0 2m29s kube-system traefik-6f9cbd9bd4-wmnh6 1/1 Running 0 2m29s Note This install is really light as it doesn't use Docker at all but only containerd Warning There is currently an issue to create an agent - and so create a k3scluster So we will work on a standalone node. woker node installation The installation is extremely straightforward You need to get the server authentication token (on box1) here: /var/lib/rancher/k3s/server/node-token sudo -i # # token comes from worker1(master) file /var/lib/rancher/k3s/server/node-token # TAKE THE FULL TOKEN !!!! # scp 10 .0.3.6:/var/lib/rancher/k3s/server/node-token /tmp/node-token export AUTH_TOKEN = $( cat /tmp/node-token ) root@box2:~# curl -sfL https://get.k3s.io | K3S_TOKEN = $AUTH_TOKEN K3S_URL = https://10.0.3.6:6443 sh - # # post-install: TO NOT FORGET # echo \" --flannel-iface 'eth1'\">>/etc/systemd/system/k3s-agent.service systemctl daemon-reload systemctl stop k3s-agent systemctl start k3s-agent Warning Take the Token from the Server node (Worker1) from the file /var/lib/rancher/k3s/server/node-token Don't trunc or alter the Token !!! At the end on the master node (box1), you should see this: root@box1:~# sudo k3s kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME box2 Ready <none> 13m v1.20.2+k3s1 10.0.3.7 <none> Ubuntu 18.04.3 LTS 4.15.0-58-generic containerd://1.4.3-k3s1 box1 Ready control-plane,master 27m v1.20.2+k3s1 10.0.3.6 <none> Ubuntu 18.04.3 LTS 4.15.0-58-generic containerd://1.4.3-k3s1 Cluster validation LABS : You have to validate the cluster is working well using This is how to check Kubernes is really working Clean up agent manually To repair bad configuration without restoring a snapshot sudo /usr/local/bin/k3s-killall.sh sudo /usr/local/bin/k3s-agent-uninstall.sh K8S major Tools installation k9s GUI cli tool k9s is a wonderful cli GUI interface. k9S url Trying it is adopting it. It makes all operations more easy on the K8S cluster. A script to install it. curl -v -L https://github.com/derailed/k9s/releases/download/v0.27.2/k9s_Linux_amd64.tar.gz -o /tmp/k9s_Linux_amd64.tar.gz cd /tmp tar -zxvf k9s_Linux_amd64.tar.gz sudo cp /tmp/k9s /usr/local/bin sudo chmod +x /usr/local/bin/k9s mkdir ~/.kube # if you want t get it remotly # scp 10.0.3.6:/etc/rancher/k3s/k3s.yaml $HOME/.kube/config cp /etc/rancher/k3s/k3s.yaml $HOME /.kube/config !!! Note: If you copy the file to another node, you need to edit the file $HOME/.kube/config and change the 127.0.0.1 by the master node IP: 10.0.3.6 K3d k3d is a lightweight wrapper to run k3s (Rancher Lab\u2019s minimal Kubernetes distribution) in docker. k3d makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes. We will use K3d instead of minikube for several reasons: We have docker desktop - so we can benefit from it minikube is single node and has not the flexibility of k3d to create many clusters K3d installation On windows: choco install K3d On Linux like: Follow this link to install latest release K3d usage # one cluster k3d cluster create demo --api-port 6550 --servers 1 --agents 3 --port 8080 :80@loadbalancer --wait # another one k3d cluster create another --api-port 7550 --servers 1 --agents 1 --port 9080 :80@loadbalancer --wait Tip install the following tools: kubectx, kubens, k9s Demo The best thing is to clone this repository: https://github.com/iwilltry42/k3d-demo RKE from Rancher RKE is to deploy efficiently a production Kubernetes cluster. Why RKE ? Read this introduction to Rke2 Tip Do a Virtualbox snapshot between all LABS ... thus you can come back to your snapshot easily. Rigor is really a good thing when talking about integration Installation curl -LO https://github.com/rancher/rke/releases/download/v1.0.16/rke_linux-amd64 sudo cp rke_linux-amd64 /usr/local/bin/rke sudo chmod +x /usr/local/bin/rke Then we have to manage the SSH communication with other nodes ssh-keygen Then we install the id_rsa.pub public key in the vagrant@worker2:/home/vagrant/.ssh/authorized_keys Note Normally all these operations are done automatically with an automation tool like ansible It's important to check manually ssh connectivity We create the cluster configuration file: cluster.yml ssh_key_path : \"/home/vagrant/.ssh/id_rsa\" ssh_cert_path : \"\" ssh_agent_auth : false nodes : - address : \"10.0.3.6\" role : - controlplane - etcd - worker user : \"vagrant\" - address : \"10.0.3.7\" role : - worker user : \"vagrant\" Tip probably it would be better to add the worker1 as a worker too ... to not spoil resources Then it's almost finished: rke up This can take a long time if your network is slow ... as it will download ~2Gb of data If everything is fine, you need just to configure kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/ ` curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ` /bin/linux/amd64/kubectl cp kubectl /usr/local/bin/kubectl sudo chmod +x /usr/local/bin/kubectl export KUBECONFIG = \"/home/vagrant/kube_config_cluster.yml\" vagrant@box1:~$ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"20\" , GitVersion: \"v1.20.2\" , GitCommit: \"faecb196815e248d3ecfb03c680a4507229c2a56\" , GitTreeState: \"clean\" , BuildDate: \"2021-01-13T13:28:09Z\" , GoVersion: \"go1.15.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"17\" , GitVersion: \"v1.17.17\" , GitCommit: \"f3abc15296f3a3f54e4ee42e830c61047b13895f\" , GitTreeState: \"clean\" , BuildDate: \"2021-01-13T13:13:00Z\" , GoVersion: \"go1.13.15\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } You are done ! vagrant@box1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .0.3.6 Ready controlplane,etcd 29m v1.17.17 10 .0.3.7 Ready worker 29m v1.17.17 Minikube (obsolete) Installation link: Minikube installation (Minikube) Windows Installation Requirements You need virtualbox or Hyper-v Installation On Windows, from an elevated command shell : choco install minikube minikube start The download ISO for minikube is around 170 Mb Info Kubectl (kubernetes client) installation no more needed , as this comes with minikube installation choco install kubernetes-cli Of course, you may need to upgrade a previous installation: choco upgrade minikube -y Recovery procedure Warning For Windows This procedure allows recovering to a sane environment and replay installation minikube stop choco uninstall minikube;kubernetes-cli Go to your USER home directory Delete ALL files under subdirectory .minikube Delete All files under subdirectory .kube (Optional) set your company proxy Install minikube choco install minikube Put minikube.exe and kubectl.exe in your windows PATH (Optional) set your company proxy minikube start Test with kubectl: kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 1m v1.10.0 Warning We have an installed version 1.10.0 because we are not based on Windows 10 and Docker for Windows So we suggest in that case to move to Linux installation","title":"Kubernetes - installation"},{"location":"day2/kubernetesInstallation/#kubernetes-installation","text":"","title":"Kubernetes installation"},{"location":"day2/kubernetesInstallation/#introduction","text":"Kubernetes can be installed in many ways. We will concentrate here on Private Home Office installation. As this is an incredibly changing world , we are testing some tools having in mind following KISS principle even for production ready cluster. Method Scope / Usage Alternative minikube Dev environment microk8s k3d or kubeadm k3s Edge computing, ARM microk8s also rke Production cluster kubeadm Warning After many test, it seems to me better and more reliable to rely on K3s for our hands-on minikube is also a good choice ... but experimented bad/strange behavior on my Windows 7 laptop","title":"Introduction"},{"location":"day2/kubernetesInstallation/#checking-installation","text":"Link: This is how to check Kubernes is really working","title":"Checking installation"},{"location":"day2/kubernetesInstallation/#k3s-from-rancher","text":"Quick Installation and architecture is here: Lightweight Kubernetes Reference documentation is here: Reference K3S documentation master k3s installation with vagrant video worker k3s installation with vagrant video K3S cluster local installation","title":"K3S from Rancher"},{"location":"day2/kubernetesInstallation/#installation-on-our-vagrant-environment","text":"Note we increased the number of core=2 and memory=2048m on worker1 WE CAN STOP DOCKER SERVICE","title":"Installation on our Vagrant environment"},{"location":"day2/kubernetesInstallation/#master-installation","text":"VBoxManage snapshot worker1 restore snap-worker1-initial VBoxManage startvm worker1 --type headless VBoxManage startvm worker2 --type headless Then ssh worker1 : # root sudo -i root@box1:~$ curl -sfL https://get.k3s.io | sh - [INFO] Finding release for channel stable [INFO] Using v1.20.2+k3s1 as release [INFO] Downloading hash https://github.com/rancher/k3s/releases/download/v1.20.2+k3s1/sha256sum-amd64.txt [INFO] Downloading binary https://github.com/rancher/k3s/releases/download/v1.20.2+k3s1/k3s [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s vagrant@box1:~$ vagrant@box1:~$ vagrant@box1:~$ sudo k3s kubectl get node NAME STATUS ROLES AGE VERSION box1 Ready control-plane,master 55s v1.20.2+k3s1 Important The vagrant installation deploy the fixed NAT address on eth0 ... but the communication interface is eth1 (10.0.3.1/24) so we need to tell to the CNI that the real network is on this interface To do this, we have to update the k3s service /etc/systemd/system/k3s.service and add this : ExecStart = /usr/local/bin/k3s \\ server \\ --flannel-iface 'eth1' A script to play this patch: echo \" --flannel-iface 'eth1'\" >>/etc/systemd/system/k3s.service systemctl daemon-reload systemctl restart k3s To get rights as vagrant user: vagrant@box1:~$ export KUBECONFIG=\"/etc/rancher/k3s/k3s.yaml\" vagrant@box1:~$ sudo chmod o+r /etc/rancher/k3s/k3s.yaml vagrant@box1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system metrics-server-86cbb8457f-45xvh 1/1 Running 0 3m11s kube-system local-path-provisioner-7c458769fb-c7rbv 1/1 Running 0 3m11s kube-system coredns-854c77959c-mh47k 1/1 Running 0 3m11s kube-system helm-install-traefik-hpjb2 0/1 Completed 0 3m11s kube-system svclb-traefik-kt4k4 2/2 Running 0 2m29s kube-system traefik-6f9cbd9bd4-wmnh6 1/1 Running 0 2m29s Note This install is really light as it doesn't use Docker at all but only containerd Warning There is currently an issue to create an agent - and so create a k3scluster So we will work on a standalone node.","title":"master installation"},{"location":"day2/kubernetesInstallation/#woker-node-installation","text":"The installation is extremely straightforward You need to get the server authentication token (on box1) here: /var/lib/rancher/k3s/server/node-token sudo -i # # token comes from worker1(master) file /var/lib/rancher/k3s/server/node-token # TAKE THE FULL TOKEN !!!! # scp 10 .0.3.6:/var/lib/rancher/k3s/server/node-token /tmp/node-token export AUTH_TOKEN = $( cat /tmp/node-token ) root@box2:~# curl -sfL https://get.k3s.io | K3S_TOKEN = $AUTH_TOKEN K3S_URL = https://10.0.3.6:6443 sh - # # post-install: TO NOT FORGET # echo \" --flannel-iface 'eth1'\">>/etc/systemd/system/k3s-agent.service systemctl daemon-reload systemctl stop k3s-agent systemctl start k3s-agent Warning Take the Token from the Server node (Worker1) from the file /var/lib/rancher/k3s/server/node-token Don't trunc or alter the Token !!! At the end on the master node (box1), you should see this: root@box1:~# sudo k3s kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME box2 Ready <none> 13m v1.20.2+k3s1 10.0.3.7 <none> Ubuntu 18.04.3 LTS 4.15.0-58-generic containerd://1.4.3-k3s1 box1 Ready control-plane,master 27m v1.20.2+k3s1 10.0.3.6 <none> Ubuntu 18.04.3 LTS 4.15.0-58-generic containerd://1.4.3-k3s1","title":"woker node installation"},{"location":"day2/kubernetesInstallation/#cluster-validation","text":"LABS : You have to validate the cluster is working well using This is how to check Kubernes is really working","title":"Cluster validation"},{"location":"day2/kubernetesInstallation/#clean-up-agent-manually","text":"To repair bad configuration without restoring a snapshot sudo /usr/local/bin/k3s-killall.sh sudo /usr/local/bin/k3s-agent-uninstall.sh","title":"Clean up agent manually"},{"location":"day2/kubernetesInstallation/#k8s-major-tools-installation","text":"","title":"K8S major Tools installation"},{"location":"day2/kubernetesInstallation/#k9s-gui-cli-tool","text":"k9s is a wonderful cli GUI interface. k9S url Trying it is adopting it. It makes all operations more easy on the K8S cluster. A script to install it. curl -v -L https://github.com/derailed/k9s/releases/download/v0.27.2/k9s_Linux_amd64.tar.gz -o /tmp/k9s_Linux_amd64.tar.gz cd /tmp tar -zxvf k9s_Linux_amd64.tar.gz sudo cp /tmp/k9s /usr/local/bin sudo chmod +x /usr/local/bin/k9s mkdir ~/.kube # if you want t get it remotly # scp 10.0.3.6:/etc/rancher/k3s/k3s.yaml $HOME/.kube/config cp /etc/rancher/k3s/k3s.yaml $HOME /.kube/config !!! Note: If you copy the file to another node, you need to edit the file $HOME/.kube/config and change the 127.0.0.1 by the master node IP: 10.0.3.6","title":"k9s GUI cli tool"},{"location":"day2/kubernetesInstallation/#k3d","text":"k3d is a lightweight wrapper to run k3s (Rancher Lab\u2019s minimal Kubernetes distribution) in docker. k3d makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes. We will use K3d instead of minikube for several reasons: We have docker desktop - so we can benefit from it minikube is single node and has not the flexibility of k3d to create many clusters","title":"K3d"},{"location":"day2/kubernetesInstallation/#k3d-installation","text":"On windows: choco install K3d On Linux like: Follow this link to install latest release","title":"K3d installation"},{"location":"day2/kubernetesInstallation/#k3d-usage","text":"# one cluster k3d cluster create demo --api-port 6550 --servers 1 --agents 3 --port 8080 :80@loadbalancer --wait # another one k3d cluster create another --api-port 7550 --servers 1 --agents 1 --port 9080 :80@loadbalancer --wait Tip install the following tools: kubectx, kubens, k9s","title":"K3d usage"},{"location":"day2/kubernetesInstallation/#demo","text":"The best thing is to clone this repository: https://github.com/iwilltry42/k3d-demo","title":"Demo"},{"location":"day2/kubernetesInstallation/#rke-from-rancher","text":"RKE is to deploy efficiently a production Kubernetes cluster. Why RKE ? Read this introduction to Rke2 Tip Do a Virtualbox snapshot between all LABS ... thus you can come back to your snapshot easily. Rigor is really a good thing when talking about integration","title":"RKE from Rancher"},{"location":"day2/kubernetesInstallation/#installation","text":"curl -LO https://github.com/rancher/rke/releases/download/v1.0.16/rke_linux-amd64 sudo cp rke_linux-amd64 /usr/local/bin/rke sudo chmod +x /usr/local/bin/rke Then we have to manage the SSH communication with other nodes ssh-keygen Then we install the id_rsa.pub public key in the vagrant@worker2:/home/vagrant/.ssh/authorized_keys Note Normally all these operations are done automatically with an automation tool like ansible It's important to check manually ssh connectivity We create the cluster configuration file: cluster.yml ssh_key_path : \"/home/vagrant/.ssh/id_rsa\" ssh_cert_path : \"\" ssh_agent_auth : false nodes : - address : \"10.0.3.6\" role : - controlplane - etcd - worker user : \"vagrant\" - address : \"10.0.3.7\" role : - worker user : \"vagrant\" Tip probably it would be better to add the worker1 as a worker too ... to not spoil resources Then it's almost finished: rke up This can take a long time if your network is slow ... as it will download ~2Gb of data If everything is fine, you need just to configure kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/ ` curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ` /bin/linux/amd64/kubectl cp kubectl /usr/local/bin/kubectl sudo chmod +x /usr/local/bin/kubectl export KUBECONFIG = \"/home/vagrant/kube_config_cluster.yml\" vagrant@box1:~$ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"20\" , GitVersion: \"v1.20.2\" , GitCommit: \"faecb196815e248d3ecfb03c680a4507229c2a56\" , GitTreeState: \"clean\" , BuildDate: \"2021-01-13T13:28:09Z\" , GoVersion: \"go1.15.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"17\" , GitVersion: \"v1.17.17\" , GitCommit: \"f3abc15296f3a3f54e4ee42e830c61047b13895f\" , GitTreeState: \"clean\" , BuildDate: \"2021-01-13T13:13:00Z\" , GoVersion: \"go1.13.15\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } You are done ! vagrant@box1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .0.3.6 Ready controlplane,etcd 29m v1.17.17 10 .0.3.7 Ready worker 29m v1.17.17","title":"Installation"},{"location":"day2/kubernetesInstallation/#minikube-obsolete","text":"Installation link: Minikube installation","title":"Minikube (obsolete)"},{"location":"day2/kubernetesInstallation/#minikube-windows-installation","text":"","title":"(Minikube) Windows Installation"},{"location":"day2/kubernetesInstallation/#requirements","text":"You need virtualbox or Hyper-v","title":"Requirements"},{"location":"day2/kubernetesInstallation/#installation_1","text":"On Windows, from an elevated command shell : choco install minikube minikube start The download ISO for minikube is around 170 Mb Info Kubectl (kubernetes client) installation no more needed , as this comes with minikube installation choco install kubernetes-cli Of course, you may need to upgrade a previous installation: choco upgrade minikube -y","title":"Installation"},{"location":"day2/kubernetesInstallation/#recovery-procedure","text":"Warning For Windows This procedure allows recovering to a sane environment and replay installation minikube stop choco uninstall minikube;kubernetes-cli Go to your USER home directory Delete ALL files under subdirectory .minikube Delete All files under subdirectory .kube (Optional) set your company proxy Install minikube choco install minikube Put minikube.exe and kubectl.exe in your windows PATH (Optional) set your company proxy minikube start Test with kubectl: kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 1m v1.10.0 Warning We have an installed version 1.10.0 because we are not based on Windows 10 and Docker for Windows So we suggest in that case to move to Linux installation","title":"Recovery procedure"},{"location":"day2/kubernetesIntroduction/","text":"Kubernetes introduction What is Kubernetes? To begin to understand the usefulness of Kubernetes, we have to first understand two concepts: immutable infrastructure and containers . Immutable infrastructure is a practice where servers, once deployed, are never modified. If something needs to be changed, you never do so directly on the server. Instead, you\u2019ll build a new server from a base image, that have all your needed changes baked in. This way we can simply replace the old server with the new one without any additional modification. Containers offer a way to package code, runtime, system tools, system libraries, and configs altogether. This shipment is a lightweight, standalone executable. This way, your application will behave the same every time no matter where it runs (e.g, Ubuntu, Windows, etc.). Containerization is not a new concept, but it has gained immense popularity with the rise of microservices and Docker. Armed with those concepts, we can now define Kubernetes as a {=container or microservice platform=} that orchestrates computing, networking, and storage infrastructure workloads. Because it doesn\u2019t limit the types of apps you can deploy (any language works), Kubernetes extends how we scale containerized applications so that we can enjoy all the benefits of a truly immutable infrastructure . The general rule of thumb for K8S: \"if your app fits in a container, Kubernetes will deploy it.\" Info By the way, if you\u2019re wondering where the name \u201cKubernetes\u201d came from, it is a Greek word, meaning helmsman or pilot . The abbreviation K8s is derived by replacing the eight letters of \u201cubernete\u201d with the digit 8. The Kubernetes Project was open-sourced by Google in 2014 after using it to run production workloads at scale for more than a decade. (Borg project) Kubernetes provides the ability to : run dynamically scaling, containerised applications, utilising an API for management. Kubernetes is a : vendor-agnostic (no lock-in) container management tool, minifying cloud computing costs whilst simplifying the running of resilient and scalable applications. Kubernetes has become the standard for running containerised applications in the cloud, with the main Cloud Providers (AWS, Azure, GCE, IBM and Oracle) now offering managed Kubernetes services. Provider Kubernetes offer AWS EKS Azure AKS GCE GKE IBM IBM Cloud Kubernetes Service Oracle OKE Scaleway Kubernetes Kapsule & Kosmos Digital Ocean Kubernetes Kapsule & Kosmos (...) (...) Kubernetes simplifies interoperability opening door to multi-cloud The success of Kubernetes From an enterprise survey (2020) done by VMware in United States, the prevailing sentiment is that Kubernetes won. 59% of the surveyed companies are using Kubernetes in prtoduction. Some takeway from the survey: why ? 56% of respondents using Kubernetes saw improved resource utilization 53% saw shortened development cycles with Kubernetes 50% found that it helped to containerize monolithic apps 42% said that it enabled the organization to move to the cloud 33% found that it reduced public cloud costs Only 5% found it was not affording an improvement. where ? 64% of respondents deploy Kubernetes on-premises 42% run it with a single, public cloud vendor 31% utilize multiple public cloud vendors 10% use distributed edge locations Kubernetes basic terms and definitions To begin understanding how to use K8S, we must understand the objects in the API. Basic K8S objects and several higher-level abstractions are known as controllers. These are the building block of your application lifecycle. Some Basic objects Pod . A group of one or more containers. Service . An abstraction that defines a logical set of pods as well as the policy for accessing them. Volume . An abstraction that lets us persist data. (This is necessary because containers are ephemeral\u2014meaning data is deleted when the container is deleted.) Namespace . A segment of the cluster dedicated to a certain purpose, for example a certain project or team of devs. Some Controllers, or higher-level abstractions ReplicaSet (RS) . Ensures the desired amount of pod is what\u2019s running. Deployment . Offers declarative updates for pods an RS. StatefulSet . A workload API object that manages stateful applications, such as databases. DaemonSet . Ensures that all or some worker nodes run a copy of a pod. This is useful for daemon applications like Fluentd, Logstash, Node Exporter ... Job . Creates one or more pods, runs a certain task(s) to completion, then deletes the pod(s). Other concepts Images : Typically a docker container image \u2013 an executable image containing everything you need to run your application; application code, libraries, a runtime, environment variables and configuration files. At runtime, a container image becomes a container which runs everything that is packaged into that image. Micro services : A specific part of a previously monolithic application. A traditional micro-service based architecture would have multiple services making up one, or more, end products. Micro services are typically shared between applications Kubernetes architecture A working Kubernetes deployment is called a cluster . You can visualize a Kubernetes cluster as two parts: the control plane and the compute machines , or worker nodes . Each node is its own Linux\u00ae environment, and could be either a physical or virtual machine. Each node runs pods , which are made up of containers . This diagram shows how the parts of a Kubernetes cluster relate to one another: Master components These master components comprise a master node: Component Role Kube-apiserver Exposes the API Etcd Key value stores all cluster data. (On master nodes or on dedicated clusters) Kube-scheduler Schedules new pods on worker nodes Kube-controller-manager Runs the controllers Node components Component Role Kubelet Agent that ensures containers in a pod are running. Kube-proxy Keeps network rules and perform forwarding Container runtime Runs containers Remark To run the containers, each compute node has a container runtime engine. Docker is one example, but Kubernetes supports other Open Container Initiative-compliant runtimes as well, such as containerd, lxc and CRI-O. What else needed? Persistent storage : Beyond just managing the containers that run an application, Kubernetes can also manage the application data attached to a cluster. Kubernetes allows users to request storage resources without having to know the details of the underlying storage infrastructure. Persistent volumes are specific to a cluster, rather than a pod, and thus can outlive the life of a pod. Container registry : The container images that Kubernetes relies on are stored in a container registry. This can be a registry you configure, or a third party registry. Underlying infrastructure : Where you run Kubernetes is up to you. This can be bare metal servers, virtual machines, public cloud providers, private clouds, and hybrid cloud environments. One of Kubernetes\u2019s key advantages is it works on many different kinds of infrastructure. What benefits does Kubernetes offer? Out of the box, K8S provides several key features that allow us to run immutable infrastructure. Containers can be killed, replaced, and self-heal automatically, and the new container gets access to those support volumes, secrets, configurations, etc., that make it function. You can have also a look to Kubernetes home page These key K8S features make your containerized application scale efficiently: Horizontal scaling . Scale your application as needed from command line or UI. Automated rollouts and rollbacks . Roll out changes that monitor the health of your application\u2014ensuring all instances don\u2019t fail or go down simultaneously. If something goes wrong, K8S automatically rolls back the change. Service discovery and load balancing . Containers get their own IP so you can put a set of containers behind a single DNS name for load balancing. Storage orchestration . Automatically mount local or public cloud or a network storage. Secret and configuration management . Create and update secrets and configs without rebuilding your image. Self-healing . The platform heals many problems: restarting failed containers, replacing and rescheduling containers as nodes die, killing containers that don\u2019t respond to your user-defined health check, and waiting to advertise containers to clients until they\u2019re ready. Batch execution . Manage your batch and Continuous Integration workloads and replace failed containers. Automatic bin packing . Automatically schedules containers based on resource requirements and other constraints. What won\u2019t Kubernetes do? Kubernetes can do a lot of cool, useful things. But it\u2019s just as important to consider what Kubernetes isn\u2019t capable of: It does not replace tools like Jenkins\u2014so it will not build your application for you. (Not a PaaS) It is not middleware\u2014so it will not perform tasks that a middleware performs, such as message bus or caching, to name a few. It does not care which logging solution is used. Have your app log to stdout, then you can collect the logs with whatever you want. It does not care about your config language (e.g., JSON). K8s is not opinionated with these things simply to allow us to build our app the way we want, expose any type of information and collect that information however we want. Kubernetes competitors Of course, Kubernetes isn\u2019t the only tool on the market. There are a variety, including: Docker Compose \u2014 good for staging but not production-ready. Docker Swarm - Docker Swarm seems easy and quick to bootstrap ... but Docker Inc. has been purchased by Mirantis - there are some concerns about the future. It's sufficient for small deployment Hashicorp Nomad \u2014 allows for cluster management and scheduling but it does not solve secret and config management, service discover , and monitoring needs. Titus\u2014Netflix\u2019s open-source orchestration platform doesn\u2019t have enough people using it in production. Overall, Kubernetes offers the best out-of-the-box features along with countless third-party add-ons to easily extend its functionality.","title":"Kubernetes - introduction"},{"location":"day2/kubernetesIntroduction/#kubernetes-introduction","text":"","title":"Kubernetes introduction"},{"location":"day2/kubernetesIntroduction/#what-is-kubernetes","text":"To begin to understand the usefulness of Kubernetes, we have to first understand two concepts: immutable infrastructure and containers . Immutable infrastructure is a practice where servers, once deployed, are never modified. If something needs to be changed, you never do so directly on the server. Instead, you\u2019ll build a new server from a base image, that have all your needed changes baked in. This way we can simply replace the old server with the new one without any additional modification. Containers offer a way to package code, runtime, system tools, system libraries, and configs altogether. This shipment is a lightweight, standalone executable. This way, your application will behave the same every time no matter where it runs (e.g, Ubuntu, Windows, etc.). Containerization is not a new concept, but it has gained immense popularity with the rise of microservices and Docker. Armed with those concepts, we can now define Kubernetes as a {=container or microservice platform=} that orchestrates computing, networking, and storage infrastructure workloads. Because it doesn\u2019t limit the types of apps you can deploy (any language works), Kubernetes extends how we scale containerized applications so that we can enjoy all the benefits of a truly immutable infrastructure . The general rule of thumb for K8S: \"if your app fits in a container, Kubernetes will deploy it.\" Info By the way, if you\u2019re wondering where the name \u201cKubernetes\u201d came from, it is a Greek word, meaning helmsman or pilot . The abbreviation K8s is derived by replacing the eight letters of \u201cubernete\u201d with the digit 8. The Kubernetes Project was open-sourced by Google in 2014 after using it to run production workloads at scale for more than a decade. (Borg project) Kubernetes provides the ability to : run dynamically scaling, containerised applications, utilising an API for management. Kubernetes is a : vendor-agnostic (no lock-in) container management tool, minifying cloud computing costs whilst simplifying the running of resilient and scalable applications. Kubernetes has become the standard for running containerised applications in the cloud, with the main Cloud Providers (AWS, Azure, GCE, IBM and Oracle) now offering managed Kubernetes services. Provider Kubernetes offer AWS EKS Azure AKS GCE GKE IBM IBM Cloud Kubernetes Service Oracle OKE Scaleway Kubernetes Kapsule & Kosmos Digital Ocean Kubernetes Kapsule & Kosmos (...) (...) Kubernetes simplifies interoperability opening door to multi-cloud","title":"What is Kubernetes?"},{"location":"day2/kubernetesIntroduction/#the-success-of-kubernetes","text":"From an enterprise survey (2020) done by VMware in United States, the prevailing sentiment is that Kubernetes won. 59% of the surveyed companies are using Kubernetes in prtoduction. Some takeway from the survey:","title":"The success of Kubernetes"},{"location":"day2/kubernetesIntroduction/#why","text":"56% of respondents using Kubernetes saw improved resource utilization 53% saw shortened development cycles with Kubernetes 50% found that it helped to containerize monolithic apps 42% said that it enabled the organization to move to the cloud 33% found that it reduced public cloud costs Only 5% found it was not affording an improvement.","title":"why ?"},{"location":"day2/kubernetesIntroduction/#where","text":"64% of respondents deploy Kubernetes on-premises 42% run it with a single, public cloud vendor 31% utilize multiple public cloud vendors 10% use distributed edge locations","title":"where ?"},{"location":"day2/kubernetesIntroduction/#kubernetes-basic-terms-and-definitions","text":"To begin understanding how to use K8S, we must understand the objects in the API. Basic K8S objects and several higher-level abstractions are known as controllers. These are the building block of your application lifecycle.","title":"Kubernetes basic terms and definitions"},{"location":"day2/kubernetesIntroduction/#some-basic-objects","text":"Pod . A group of one or more containers. Service . An abstraction that defines a logical set of pods as well as the policy for accessing them. Volume . An abstraction that lets us persist data. (This is necessary because containers are ephemeral\u2014meaning data is deleted when the container is deleted.) Namespace . A segment of the cluster dedicated to a certain purpose, for example a certain project or team of devs.","title":"Some Basic objects"},{"location":"day2/kubernetesIntroduction/#some-controllers-or-higher-level-abstractions","text":"ReplicaSet (RS) . Ensures the desired amount of pod is what\u2019s running. Deployment . Offers declarative updates for pods an RS. StatefulSet . A workload API object that manages stateful applications, such as databases. DaemonSet . Ensures that all or some worker nodes run a copy of a pod. This is useful for daemon applications like Fluentd, Logstash, Node Exporter ... Job . Creates one or more pods, runs a certain task(s) to completion, then deletes the pod(s).","title":"Some Controllers, or higher-level abstractions"},{"location":"day2/kubernetesIntroduction/#other-concepts","text":"Images : Typically a docker container image \u2013 an executable image containing everything you need to run your application; application code, libraries, a runtime, environment variables and configuration files. At runtime, a container image becomes a container which runs everything that is packaged into that image. Micro services : A specific part of a previously monolithic application. A traditional micro-service based architecture would have multiple services making up one, or more, end products. Micro services are typically shared between applications","title":"Other concepts"},{"location":"day2/kubernetesIntroduction/#kubernetes-architecture","text":"A working Kubernetes deployment is called a cluster . You can visualize a Kubernetes cluster as two parts: the control plane and the compute machines , or worker nodes . Each node is its own Linux\u00ae environment, and could be either a physical or virtual machine. Each node runs pods , which are made up of containers . This diagram shows how the parts of a Kubernetes cluster relate to one another:","title":"Kubernetes architecture"},{"location":"day2/kubernetesIntroduction/#master-components","text":"These master components comprise a master node: Component Role Kube-apiserver Exposes the API Etcd Key value stores all cluster data. (On master nodes or on dedicated clusters) Kube-scheduler Schedules new pods on worker nodes Kube-controller-manager Runs the controllers","title":"Master components"},{"location":"day2/kubernetesIntroduction/#node-components","text":"Component Role Kubelet Agent that ensures containers in a pod are running. Kube-proxy Keeps network rules and perform forwarding Container runtime Runs containers Remark To run the containers, each compute node has a container runtime engine. Docker is one example, but Kubernetes supports other Open Container Initiative-compliant runtimes as well, such as containerd, lxc and CRI-O.","title":"Node components"},{"location":"day2/kubernetesIntroduction/#what-else-needed","text":"Persistent storage : Beyond just managing the containers that run an application, Kubernetes can also manage the application data attached to a cluster. Kubernetes allows users to request storage resources without having to know the details of the underlying storage infrastructure. Persistent volumes are specific to a cluster, rather than a pod, and thus can outlive the life of a pod. Container registry : The container images that Kubernetes relies on are stored in a container registry. This can be a registry you configure, or a third party registry. Underlying infrastructure : Where you run Kubernetes is up to you. This can be bare metal servers, virtual machines, public cloud providers, private clouds, and hybrid cloud environments. One of Kubernetes\u2019s key advantages is it works on many different kinds of infrastructure.","title":"What else needed?"},{"location":"day2/kubernetesIntroduction/#what-benefits-does-kubernetes-offer","text":"Out of the box, K8S provides several key features that allow us to run immutable infrastructure. Containers can be killed, replaced, and self-heal automatically, and the new container gets access to those support volumes, secrets, configurations, etc., that make it function. You can have also a look to Kubernetes home page These key K8S features make your containerized application scale efficiently: Horizontal scaling . Scale your application as needed from command line or UI. Automated rollouts and rollbacks . Roll out changes that monitor the health of your application\u2014ensuring all instances don\u2019t fail or go down simultaneously. If something goes wrong, K8S automatically rolls back the change. Service discovery and load balancing . Containers get their own IP so you can put a set of containers behind a single DNS name for load balancing. Storage orchestration . Automatically mount local or public cloud or a network storage. Secret and configuration management . Create and update secrets and configs without rebuilding your image. Self-healing . The platform heals many problems: restarting failed containers, replacing and rescheduling containers as nodes die, killing containers that don\u2019t respond to your user-defined health check, and waiting to advertise containers to clients until they\u2019re ready. Batch execution . Manage your batch and Continuous Integration workloads and replace failed containers. Automatic bin packing . Automatically schedules containers based on resource requirements and other constraints.","title":"What benefits does Kubernetes offer?"},{"location":"day2/kubernetesIntroduction/#what-wont-kubernetes-do","text":"Kubernetes can do a lot of cool, useful things. But it\u2019s just as important to consider what Kubernetes isn\u2019t capable of: It does not replace tools like Jenkins\u2014so it will not build your application for you. (Not a PaaS) It is not middleware\u2014so it will not perform tasks that a middleware performs, such as message bus or caching, to name a few. It does not care which logging solution is used. Have your app log to stdout, then you can collect the logs with whatever you want. It does not care about your config language (e.g., JSON). K8s is not opinionated with these things simply to allow us to build our app the way we want, expose any type of information and collect that information however we want.","title":"What won\u2019t Kubernetes do?"},{"location":"day2/kubernetesIntroduction/#kubernetes-competitors","text":"Of course, Kubernetes isn\u2019t the only tool on the market. There are a variety, including: Docker Compose \u2014 good for staging but not production-ready. Docker Swarm - Docker Swarm seems easy and quick to bootstrap ... but Docker Inc. has been purchased by Mirantis - there are some concerns about the future. It's sufficient for small deployment Hashicorp Nomad \u2014 allows for cluster management and scheduling but it does not solve secret and config management, service discover , and monitoring needs. Titus\u2014Netflix\u2019s open-source orchestration platform doesn\u2019t have enough people using it in production. Overall, Kubernetes offers the best out-of-the-box features along with countless third-party add-ons to easily extend its functionality.","title":"Kubernetes competitors"},{"location":"day2/kubernetesNetwork/","text":"Kubernetes Networking Remark Credit for all these beautiful image is the OVH article at this Url Image are generally more clear than long explanation :Angel: ClusterIP All pods are reachable thru a service. By default a service is an internal IP named ClusterIP. You can reach this IP: * Internally from another pod, for instance a busybox instance * Externally using the Kubectl proxy command (but suitable only for development) This schema summarize this second approach: Remark Pods Cidr & Service network Cidr are strictly separated Important Never try to test a network service with a ping - it won't work Test it using one of the following tool: wget, curl, netcat ... NodePort Nodeports allows to expose permanently an internal port. Once declared, Nodeport are reachable on all Worker nodes This schema shows the NodePort concept: LoadBalancer Nodeport are bound to each node and are statically adressed by external client (Browser for instance) To Load balance access to node ports from an external application, you need to create a Load balancer Ip . This schema shows the Load Balancer concept: Remark Exposing service on a Load balancer IP is a Cloud provider proposed feature (EKS, AKS, GKE, ... ) Even if it's practical, you have to pay for each fixed IP provided even if you ask for an internal IP (and not a public exposed IP) To bypass this constraint, you may deploy an internal Kubernetes rounting service named \"Ingres Controler\" We will see an example of this later based on \"Traefik\". Tip Even on a Private Cloud, you can integrate your own Load Balancer MetalLB is an example of such possibility You need to book some IPs in your Infrastructure for this usage. Ingress Controler As explained just above, Ingress Controler allows to route call to a bunch of internal services using naming routing ( prefix based, regex based ...) The main advantages are: * Reduce the need for Load Balancer * Allows masquerading the internal Url naming by presenting external names for API * Do some pre-processing before routing to internal services (Authorization for instance) This schema shows the Ingress Controler concept: Service Mesch Service mesch are the possibility to manage service access by adding high value features like: Canary release A/B testing ( exposing 2 differents Web sites to 2 different users group) Rate limiting Access Control end-to-end authentication (by injecting SSL ) Well known service mesch are Istio and Linkerd .","title":"Kubernetes - networking concepts"},{"location":"day2/kubernetesNetwork/#kubernetes-networking","text":"Remark Credit for all these beautiful image is the OVH article at this Url Image are generally more clear than long explanation :Angel:","title":"Kubernetes Networking"},{"location":"day2/kubernetesNetwork/#clusterip","text":"All pods are reachable thru a service. By default a service is an internal IP named ClusterIP. You can reach this IP: * Internally from another pod, for instance a busybox instance * Externally using the Kubectl proxy command (but suitable only for development) This schema summarize this second approach: Remark Pods Cidr & Service network Cidr are strictly separated Important Never try to test a network service with a ping - it won't work Test it using one of the following tool: wget, curl, netcat ...","title":"ClusterIP"},{"location":"day2/kubernetesNetwork/#nodeport","text":"Nodeports allows to expose permanently an internal port. Once declared, Nodeport are reachable on all Worker nodes This schema shows the NodePort concept:","title":"NodePort"},{"location":"day2/kubernetesNetwork/#loadbalancer","text":"Nodeport are bound to each node and are statically adressed by external client (Browser for instance) To Load balance access to node ports from an external application, you need to create a Load balancer Ip . This schema shows the Load Balancer concept: Remark Exposing service on a Load balancer IP is a Cloud provider proposed feature (EKS, AKS, GKE, ... ) Even if it's practical, you have to pay for each fixed IP provided even if you ask for an internal IP (and not a public exposed IP) To bypass this constraint, you may deploy an internal Kubernetes rounting service named \"Ingres Controler\" We will see an example of this later based on \"Traefik\". Tip Even on a Private Cloud, you can integrate your own Load Balancer MetalLB is an example of such possibility You need to book some IPs in your Infrastructure for this usage.","title":"LoadBalancer"},{"location":"day2/kubernetesNetwork/#ingress-controler","text":"As explained just above, Ingress Controler allows to route call to a bunch of internal services using naming routing ( prefix based, regex based ...) The main advantages are: * Reduce the need for Load Balancer * Allows masquerading the internal Url naming by presenting external names for API * Do some pre-processing before routing to internal services (Authorization for instance) This schema shows the Ingress Controler concept:","title":"Ingress Controler"},{"location":"day2/kubernetesNetwork/#service-mesch","text":"Service mesch are the possibility to manage service access by adding high value features like: Canary release A/B testing ( exposing 2 differents Web sites to 2 different users group) Rate limiting Access Control end-to-end authentication (by injecting SSL ) Well known service mesch are Istio and Linkerd .","title":"Service Mesch"},{"location":"day3/GoogleCloudPlatform/","text":"An introduction to Google Cloud Platform In the manifest/ subdirectory there are the files for kubernetes deployment We explain here the deployment on GCP pre-requisites Project must be created kubectl GKE api must be enabled gcloud tool must be installed For this purpose we create a dedicated box with Vagrant named controller gcloud installation curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-327.0.0-linux-x86_64.tar.gz tar zxvf google-cloud-sdk-327.0.0-linux-x86_64.tar.gz ./google-cloud-sdk/install.sh # # This command map with Oauth2 with your google account but also to project, region & zone # ./google-cloud-sdk/bin/gcloud init sudo /opt/google-cloud-sdk/bin/gcloud components install kubectl To review configuration: cat .config/gcloud/configurations/config_default [core] account = omerlin13@gmail.com project = quizz022021 [compute] zone = europe-west1-b region = europe-west1 You need first a project code, a region and zone export PROJECT=quizz022021 export REGION=europe-west1 export ZONE=europe-west1-b First we set up gcloud to connect to the gcloud project : gcloud config set project $PROJECT (we potentially have to set the kubectl context if we are multi project ) kubectl config get-contexts gcloud container clusters get-credentials test-cluster # From: ~/.kube/config MYCONTEXT=gke_mytestproject-225717_europe-west1-b_test-cluster kubectl config set-context $MYCONTEXT Kubernetes setup gcloud container clusters create test-cluster --num-nodes=3 --enable-ip-alias gcloud compute --project=$PROJECT disks create redis-disk --zone=$ZONE --type=pd-ssd --size=1GB Build de container : (cloud build api must be authorized) git clone https://github.com/omerlin/kubquizz.git cd kubquizz/server gcloud builds submit --tag eu.gcr.io/$PROJECT/srvquizz:v1 . ID CREATE_TIME DURATION SOURCE IMAGES STATUS c411e802-5e1a-47b3-87ed-06ba2dce0eff 2021-02-12T09:50:56+00:00 47S gs://quizz022021_cloudbuild/source/1613123410.312352-511918710133477cbee02f257a9d959c.tgz eu.gcr.io/quizz022021/srvquizz:v1 SUCCESS This create an image in the form : eu.gcr.io/$PROJECT/srvquizz:v1 Optional: booking the External floating IP REMARK: This is not necessary, the service will provide one IP (sufficient in our case) gcloud compute addresses create srvapp-ip --region $REGION gcloud compute addresses describe srvapp-ip --region $REGION Then add the external IP in the srvapp-service.yml LoadBalancer configuration Install srvapp application cd ~/kubquizz/server kubectl create configmap quizz-data --from-file=./resources/quizz.yml kubectl create -f manifest/cm_redishost.yml kubectl create -f manifest/redis.yml kubectl create -f manifest/srvapp.yml kubectl create -f manifest/srvapp-service.yml vagrant@controller:~/kubquizz/server$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.120.0.1 <none> 443/TCP 137m redis ClusterIP None <none> 6379/TCP 41m srvquizz LoadBalancer 10.120.8.24 35.205.41.166 80:32640/TCP 12m mapping with gandi.net quizz A 1800 35.205.41.166 so ... now the server is accessible over internet on http://quizz.cossme.pw","title":"Cloud - Google Cloud Platform"},{"location":"day3/GoogleCloudPlatform/#an-introduction-to-google-cloud-platform","text":"In the manifest/ subdirectory there are the files for kubernetes deployment We explain here the deployment on GCP","title":"An introduction to Google Cloud Platform"},{"location":"day3/GoogleCloudPlatform/#pre-requisites","text":"Project must be created kubectl GKE api must be enabled gcloud tool must be installed For this purpose we create a dedicated box with Vagrant named controller","title":"pre-requisites"},{"location":"day3/GoogleCloudPlatform/#gcloud-installation","text":"curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-327.0.0-linux-x86_64.tar.gz tar zxvf google-cloud-sdk-327.0.0-linux-x86_64.tar.gz ./google-cloud-sdk/install.sh # # This command map with Oauth2 with your google account but also to project, region & zone # ./google-cloud-sdk/bin/gcloud init sudo /opt/google-cloud-sdk/bin/gcloud components install kubectl To review configuration: cat .config/gcloud/configurations/config_default [core] account = omerlin13@gmail.com project = quizz022021 [compute] zone = europe-west1-b region = europe-west1 You need first a project code, a region and zone export PROJECT=quizz022021 export REGION=europe-west1 export ZONE=europe-west1-b First we set up gcloud to connect to the gcloud project : gcloud config set project $PROJECT (we potentially have to set the kubectl context if we are multi project ) kubectl config get-contexts gcloud container clusters get-credentials test-cluster # From: ~/.kube/config MYCONTEXT=gke_mytestproject-225717_europe-west1-b_test-cluster kubectl config set-context $MYCONTEXT","title":"gcloud installation"},{"location":"day3/GoogleCloudPlatform/#kubernetes-setup","text":"gcloud container clusters create test-cluster --num-nodes=3 --enable-ip-alias gcloud compute --project=$PROJECT disks create redis-disk --zone=$ZONE --type=pd-ssd --size=1GB","title":"Kubernetes setup"},{"location":"day3/GoogleCloudPlatform/#build-de-container","text":"(cloud build api must be authorized) git clone https://github.com/omerlin/kubquizz.git cd kubquizz/server gcloud builds submit --tag eu.gcr.io/$PROJECT/srvquizz:v1 . ID CREATE_TIME DURATION SOURCE IMAGES STATUS c411e802-5e1a-47b3-87ed-06ba2dce0eff 2021-02-12T09:50:56+00:00 47S gs://quizz022021_cloudbuild/source/1613123410.312352-511918710133477cbee02f257a9d959c.tgz eu.gcr.io/quizz022021/srvquizz:v1 SUCCESS This create an image in the form : eu.gcr.io/$PROJECT/srvquizz:v1","title":"Build de container :"},{"location":"day3/GoogleCloudPlatform/#optional-booking-the-external-floating-ip","text":"REMARK: This is not necessary, the service will provide one IP (sufficient in our case) gcloud compute addresses create srvapp-ip --region $REGION gcloud compute addresses describe srvapp-ip --region $REGION Then add the external IP in the srvapp-service.yml LoadBalancer configuration","title":"Optional: booking the External floating IP"},{"location":"day3/GoogleCloudPlatform/#install-srvapp-application","text":"cd ~/kubquizz/server kubectl create configmap quizz-data --from-file=./resources/quizz.yml kubectl create -f manifest/cm_redishost.yml kubectl create -f manifest/redis.yml kubectl create -f manifest/srvapp.yml kubectl create -f manifest/srvapp-service.yml vagrant@controller:~/kubquizz/server$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.120.0.1 <none> 443/TCP 137m redis ClusterIP None <none> 6379/TCP 41m srvquizz LoadBalancer 10.120.8.24 35.205.41.166 80:32640/TCP 12m","title":"Install srvapp application"},{"location":"day3/GoogleCloudPlatform/#mapping-with-gandinet","text":"quizz A 1800 35.205.41.166 so ... now the server is accessible over internet on http://quizz.cossme.pw","title":"mapping with gandi.net"},{"location":"day3/day2Summary/","text":"Quick Day2 summary Seen so far ... Topics discussed Mainly Day 1 Virtualization Practical on Virtualbox with Vagrant Vagrant Box Vagrant automation using scripts Networking concepts Networking deployment with Internal Network + Nat network Networking deployment with HostNetwork + Nat network Multi machine configuration WSL2 installation on Windwos 10 & Linux WSL deployment Docker Desktop Automation Introduction to Ansible Cloud Vocabulary / terms / History / Openstack / Kubernetes Day2 Docker Docker network & volume Docker build & slimming Docker push in public repository Docker compose On a complex deployment On a simple based on nodejs + mySQL Kubernetes Installation Rancher K3S cluster with a master/slave deployment K3d to replace minikube & deploy several clusters in parallel Not done: RKE & minikube Concepts Architecture & main components Services, NodePort, Ingres Controler, Load balancer Deployment debugging a kubernetes installation Simple application nodeJS + DB MySQL Material Yncrea virtualization materials for labs This course","title":"Day2 - quick summary"},{"location":"day3/day2Summary/#quick-day2-summary","text":"","title":"Quick Day2 summary"},{"location":"day3/day2Summary/#seen-so-far","text":"Topics discussed","title":"Seen so far ..."},{"location":"day3/day2Summary/#mainly-day-1","text":"","title":"Mainly Day 1"},{"location":"day3/day2Summary/#virtualization","text":"Practical on Virtualbox with Vagrant Vagrant Box Vagrant automation using scripts Networking concepts Networking deployment with Internal Network + Nat network Networking deployment with HostNetwork + Nat network Multi machine configuration WSL2 installation on Windwos 10 & Linux WSL deployment Docker Desktop Automation Introduction to Ansible","title":"Virtualization"},{"location":"day3/day2Summary/#cloud","text":"Vocabulary / terms / History / Openstack / Kubernetes","title":"Cloud"},{"location":"day3/day2Summary/#day2","text":"","title":"Day2"},{"location":"day3/day2Summary/#docker","text":"Docker network & volume Docker build & slimming Docker push in public repository Docker compose On a complex deployment On a simple based on nodejs + mySQL","title":"Docker"},{"location":"day3/day2Summary/#kubernetes","text":"Installation Rancher K3S cluster with a master/slave deployment K3d to replace minikube & deploy several clusters in parallel Not done: RKE & minikube Concepts Architecture & main components Services, NodePort, Ingres Controler, Load balancer Deployment debugging a kubernetes installation Simple application nodeJS + DB MySQL","title":"Kubernetes"},{"location":"day3/day2Summary/#material","text":"Yncrea virtualization materials for labs This course","title":"Material"},{"location":"day3/kubernetesAdvanced/","text":"Kubernetes Advanced Ok we will continue with our K3S cluster Routing service with ingress So we have installed a K3S Kubernetes that comes with pre-installed feature. One is the reverse proxy [Traefik] ( https://doc.traefik.io/traefik/ ) From the documentation: Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. Directing traffic from external clients to the containers within the cloud, while ensuring the external client remains agnostic to the cloud, is a recurring problem. A common solution is to create an ingress controller . The ingress controller is delegated the responsibility of taking incoming traffic from an external client, and determining to which container the traffic should be directed. Terminology term Meaning Ingress A Kubernetes Ingress exposes HTTP and HTTPS traffic from outside the Cluster to Services within the cluster. Ingress Controller An application that is responsible for fulfilling Ingress requests. Service A Kubernetes Service is an abstract way to expose an application running in a set of Pods. Implementations of a Service include NodePort and ClusterIP. Configuring Traefik dashboard K3s creates a Traefik deployment for the Ingress Controller, but by default, the dashboard is disabled. Running Traefik with the dashboard enabled materializes the concept of routing rules Enabling Traefik dashboard The ConfigMap for Traefik must be edited to enable the dashboard kubectl -n kube-system edit cm traefik just after the entry [Trafic_log], add this [api] dashboard = true You need to restart Traefik: kubectl -n kube-system scale deploy traefik --replicas 0 kubectl -n kube-system scale deploy traefik --replicas 1 You must expose the Dashboard port: kubectl -n kube-system port-forward deployment/traefik 8089:8080 Note Port forward is another method to access to an application inside the cluster Open the dashboard in your browser at http://localhost:8089 . our routing rules will show up on this dashboard as you create Ingress. Configure Traefik Routing Rules From the Git resource https://github.com/omerlin/yncrea-virtualization-labs.git , go to Kubernetes Ingress directory Create a deployment This is the helloworld deployment we have seen Day 2. kubectl create -f deployment.yml Create a service Instead of using the kubectl expose command, we declare the service explicitly. File service.yml : apiVersion : v1 kind : Service metadata : name : helloworld-svc spec : ports : - name : http port : 3000 selector : # apply service to any pod with label app: helloworld app : helloworld Note The service mapp application with the label: app: helloworld Create a service The Ingress configures Traefik with routing rules. This minimal example will use a path based routing rule. A path based routing rule is evaluated by inspecting the incoming url\u2019s context. Here, the path is / with pathType: Prefix. The path / captures all incoming traffic apiVersion : extensions/v1beta1 kind : Ingress metadata : name : helloworld-ingress annotations : kubernetes.io/ingress.class : traefik spec : rules : - http : paths : - path : / pathType : Prefix backend : serviceName : helloworld-svc servicePort : 3000 Routing rules will now be visible on Traefik\u2019s dashboard. Accessing the service In fact there is no need to declare a service in front of Traefic, the ingress HTTP traffic is already exposed on ALL K3S cluster by a load balancer vagrant@box1:~/yncrea-virtualization-labs/kubernetes/ingres$ kubectl -n kube-system get svc traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE traefik LoadBalancer 10 .43.50.216 10 .0.3.6 80 :31367/TCP,443:31319/TCP 7h48m So by accessing externally to port 31367 we can reach the service thru the ingress controller. The External-ip here is not routable ... sdo you have to map port at the VirtualBox level. LABS : Deploying with Traefik This Lab will deploy other routes on Traefik with another Nginx POD. The code is still located here: https://github.com/omerlin/yncrea-virtualization-labs.git Go to the sub-directory: kubernetes/ingressing_with_k3s-master Objectives are to be able to follow the instructions of this tutorial by yourself. The tutorial is about Directing Kubernetes traffic with Traefik This will show you how to redirect based on path. Important you have to take care at the config maps ... as pods need it in their configuration you should know also the command: kubectl delete -f ... to remove a LABS : Simple hello-python deployment Comes from: https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/ The idea is not to follow blindly what you see there ... First, from what you have learned so far ... what seems bad for you ??? Second, this will not work and you will have to adapt ... could you figure out why ? If you want to the the python code directly, you need to have pip to install dependencies: sudo apt-get install -y python-pip Attention As you are under K3S with default configuration, you cannot use local docker image The only solution but private registries is to push image in a public repository ( hub.docker.com is the most common) and then in the Kubernetes Yaml deployment file, change the imagePullPolicy to imagePullPolicy: IfNotPresent Kubernetes dashboard Just for fun, we will deploy the Kubernetes dashboard This will also introduce some concepts related to security in Kubernetes Official documentation is here Deploy the dashboard GITHUB_URL = https://github.com/kubernetes/dashboard/releases VERSION_KUBE_DASHBOARD = $( curl -w '%{url_effective}' -I -L -s -S ${ GITHUB_URL } /latest -o /dev/null | sed -e 's|.*/||' ) #sudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/ ${ VERSION_KUBE_DASHBOARD } /aio/deploy/recommended.yaml Deploy service account and role Dashboards are created in a dedicated namespace kubernetes-dashboard Access control in Kubernetes is managed by a feature named RBAC for Role Based Authentication Control. We need to have service account to access the dashboard. File admin-service-user.yaml : apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard For this user to have full access to the dashboard in namespace kubernetes-dashboard , we need to give a cluster role access. File admin-user-role.yaml : apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard To create the resources : kubectl create -f admin-service-user.yaml kubectl create -f admin-user-role.yaml This is Ok, but how to see the dashboard ? One solution we have seen so far is to expose the service port with the NodePort . This is not the best option, but let use it ... kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard There you may change the type: ClusterIP to type: NodePort . Save it. You are done. You can see the external port with this command : vagrant@box1:~$ kubectl get svc kubernetes-dashboard -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.43.206.84 <none> 443:31123/TCP 37m In this example, you see the port is 31123 Ok - you are almost done ... You need first get the admin-user authentication token by this command: sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token There are standard authentication token as you can see on https://jwt.io/ You so can see the kubernetes dashboard opening your browser to https://localhost:31123","title":"Kubernetes - Advanced"},{"location":"day3/kubernetesAdvanced/#kubernetes-advanced","text":"Ok we will continue with our K3S cluster","title":"Kubernetes Advanced"},{"location":"day3/kubernetesAdvanced/#routing-service-with-ingress","text":"So we have installed a K3S Kubernetes that comes with pre-installed feature. One is the reverse proxy [Traefik] ( https://doc.traefik.io/traefik/ ) From the documentation: Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. Directing traffic from external clients to the containers within the cloud, while ensuring the external client remains agnostic to the cloud, is a recurring problem. A common solution is to create an ingress controller . The ingress controller is delegated the responsibility of taking incoming traffic from an external client, and determining to which container the traffic should be directed.","title":"Routing service with ingress"},{"location":"day3/kubernetesAdvanced/#terminology","text":"term Meaning Ingress A Kubernetes Ingress exposes HTTP and HTTPS traffic from outside the Cluster to Services within the cluster. Ingress Controller An application that is responsible for fulfilling Ingress requests. Service A Kubernetes Service is an abstract way to expose an application running in a set of Pods. Implementations of a Service include NodePort and ClusterIP.","title":"Terminology"},{"location":"day3/kubernetesAdvanced/#configuring-traefik-dashboard","text":"K3s creates a Traefik deployment for the Ingress Controller, but by default, the dashboard is disabled. Running Traefik with the dashboard enabled materializes the concept of routing rules Enabling Traefik dashboard The ConfigMap for Traefik must be edited to enable the dashboard kubectl -n kube-system edit cm traefik just after the entry [Trafic_log], add this [api] dashboard = true You need to restart Traefik: kubectl -n kube-system scale deploy traefik --replicas 0 kubectl -n kube-system scale deploy traefik --replicas 1 You must expose the Dashboard port: kubectl -n kube-system port-forward deployment/traefik 8089:8080 Note Port forward is another method to access to an application inside the cluster Open the dashboard in your browser at http://localhost:8089 . our routing rules will show up on this dashboard as you create Ingress.","title":"Configuring Traefik dashboard"},{"location":"day3/kubernetesAdvanced/#configure-traefik-routing-rules","text":"From the Git resource https://github.com/omerlin/yncrea-virtualization-labs.git , go to Kubernetes Ingress directory","title":"Configure Traefik Routing Rules"},{"location":"day3/kubernetesAdvanced/#create-a-deployment","text":"This is the helloworld deployment we have seen Day 2. kubectl create -f deployment.yml","title":"Create a deployment"},{"location":"day3/kubernetesAdvanced/#create-a-service","text":"Instead of using the kubectl expose command, we declare the service explicitly. File service.yml : apiVersion : v1 kind : Service metadata : name : helloworld-svc spec : ports : - name : http port : 3000 selector : # apply service to any pod with label app: helloworld app : helloworld Note The service mapp application with the label: app: helloworld","title":"Create a service"},{"location":"day3/kubernetesAdvanced/#create-a-service_1","text":"The Ingress configures Traefik with routing rules. This minimal example will use a path based routing rule. A path based routing rule is evaluated by inspecting the incoming url\u2019s context. Here, the path is / with pathType: Prefix. The path / captures all incoming traffic apiVersion : extensions/v1beta1 kind : Ingress metadata : name : helloworld-ingress annotations : kubernetes.io/ingress.class : traefik spec : rules : - http : paths : - path : / pathType : Prefix backend : serviceName : helloworld-svc servicePort : 3000 Routing rules will now be visible on Traefik\u2019s dashboard.","title":"Create a service"},{"location":"day3/kubernetesAdvanced/#accessing-the-service","text":"In fact there is no need to declare a service in front of Traefic, the ingress HTTP traffic is already exposed on ALL K3S cluster by a load balancer vagrant@box1:~/yncrea-virtualization-labs/kubernetes/ingres$ kubectl -n kube-system get svc traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE traefik LoadBalancer 10 .43.50.216 10 .0.3.6 80 :31367/TCP,443:31319/TCP 7h48m So by accessing externally to port 31367 we can reach the service thru the ingress controller. The External-ip here is not routable ... sdo you have to map port at the VirtualBox level.","title":"Accessing the service"},{"location":"day3/kubernetesAdvanced/#labs-deploying-with-traefik","text":"This Lab will deploy other routes on Traefik with another Nginx POD. The code is still located here: https://github.com/omerlin/yncrea-virtualization-labs.git Go to the sub-directory: kubernetes/ingressing_with_k3s-master Objectives are to be able to follow the instructions of this tutorial by yourself. The tutorial is about Directing Kubernetes traffic with Traefik This will show you how to redirect based on path. Important you have to take care at the config maps ... as pods need it in their configuration you should know also the command: kubectl delete -f ... to remove a","title":"LABS: Deploying with Traefik"},{"location":"day3/kubernetesAdvanced/#labs-simple-hello-python-deployment","text":"Comes from: https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/ The idea is not to follow blindly what you see there ... First, from what you have learned so far ... what seems bad for you ??? Second, this will not work and you will have to adapt ... could you figure out why ? If you want to the the python code directly, you need to have pip to install dependencies: sudo apt-get install -y python-pip Attention As you are under K3S with default configuration, you cannot use local docker image The only solution but private registries is to push image in a public repository ( hub.docker.com is the most common) and then in the Kubernetes Yaml deployment file, change the imagePullPolicy to imagePullPolicy: IfNotPresent","title":"LABS: Simple hello-python deployment"},{"location":"day3/kubernetesAdvanced/#kubernetes-dashboard","text":"Just for fun, we will deploy the Kubernetes dashboard This will also introduce some concepts related to security in Kubernetes Official documentation is here","title":"Kubernetes dashboard"},{"location":"day3/kubernetesAdvanced/#deploy-the-dashboard","text":"GITHUB_URL = https://github.com/kubernetes/dashboard/releases VERSION_KUBE_DASHBOARD = $( curl -w '%{url_effective}' -I -L -s -S ${ GITHUB_URL } /latest -o /dev/null | sed -e 's|.*/||' ) #sudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/ ${ VERSION_KUBE_DASHBOARD } /aio/deploy/recommended.yaml","title":"Deploy the dashboard"},{"location":"day3/kubernetesAdvanced/#deploy-service-account-and-role","text":"Dashboards are created in a dedicated namespace kubernetes-dashboard Access control in Kubernetes is managed by a feature named RBAC for Role Based Authentication Control. We need to have service account to access the dashboard. File admin-service-user.yaml : apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kubernetes-dashboard For this user to have full access to the dashboard in namespace kubernetes-dashboard , we need to give a cluster role access. File admin-user-role.yaml : apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kubernetes-dashboard To create the resources : kubectl create -f admin-service-user.yaml kubectl create -f admin-user-role.yaml This is Ok, but how to see the dashboard ? One solution we have seen so far is to expose the service port with the NodePort . This is not the best option, but let use it ... kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard There you may change the type: ClusterIP to type: NodePort . Save it. You are done. You can see the external port with this command : vagrant@box1:~$ kubectl get svc kubernetes-dashboard -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.43.206.84 <none> 443:31123/TCP 37m In this example, you see the port is 31123 Ok - you are almost done ... You need first get the admin-user authentication token by this command: sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token There are standard authentication token as you can see on https://jwt.io/ You so can see the kubernetes dashboard opening your browser to https://localhost:31123","title":"Deploy service account and role"},{"location":"day3/quizz/","text":"Quizz time You are now under pressure ...but don't panic ... This is not difficult (less than what we have seen so far) Architecture Your examination Summary You install a client Quizz frontend application and then you execute the Quizz Sequence Diagram Detail The application is located in the repository Go kub quizz So you need to do a : git pull You will need to build your own docker Python container locally. This means you will have to create a Dockerfile . Then you will publish this container to your own account on http://hub.docker.com Then you create the manifest Yaml file to deploy this container as a pod on your K3S cluster. ( Option ) You can package it as a Helm package Having done this you will need to access the service port to launch a Quizz. Something like: http://localhost:34600/quizz You execute the quizz until the end ..., and you are done ! Information about this small application You have requirements on Python to manage. Your container expects the file config.yml to be in the directory /etc/quizz Important you need to change your user name in the config.yml with your Yncrea email . Info Information are exchanged with the server located on http://quizz.cossme.pw alternative You can simply deploy on Docker as this is a very simple application ... ... But be aware you loose 3 points doing that ... K3S deployment is the right way to go.","title":"Evaluation - Quizz"},{"location":"day3/quizz/#quizz-time","text":"You are now under pressure ...but don't panic ... This is not difficult (less than what we have seen so far)","title":"Quizz time"},{"location":"day3/quizz/#architecture","text":"","title":"Architecture"},{"location":"day3/quizz/#your-examination","text":"","title":"Your examination"},{"location":"day3/quizz/#summary","text":"You install a client Quizz frontend application and then you execute the Quizz","title":"Summary"},{"location":"day3/quizz/#sequence-diagram","text":"","title":"Sequence Diagram"},{"location":"day3/quizz/#detail","text":"The application is located in the repository Go kub quizz So you need to do a : git pull You will need to build your own docker Python container locally. This means you will have to create a Dockerfile . Then you will publish this container to your own account on http://hub.docker.com Then you create the manifest Yaml file to deploy this container as a pod on your K3S cluster. ( Option ) You can package it as a Helm package Having done this you will need to access the service port to launch a Quizz. Something like: http://localhost:34600/quizz You execute the quizz until the end ..., and you are done !","title":"Detail"},{"location":"day3/quizz/#information-about-this-small-application","text":"You have requirements on Python to manage. Your container expects the file config.yml to be in the directory /etc/quizz Important you need to change your user name in the config.yml with your Yncrea email . Info Information are exchanged with the server located on http://quizz.cossme.pw","title":"Information about this small application"},{"location":"day3/quizz/#alternative","text":"You can simply deploy on Docker as this is a very simple application ... ... But be aware you loose 3 points doing that ... K3S deployment is the right way to go.","title":"alternative"}]}